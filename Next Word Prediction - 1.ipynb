{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-07T10:02:49.482076Z",
     "start_time": "2020-11-07T10:02:49.474099Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T11:20:26.687208Z",
     "start_time": "2020-11-05T11:20:26.682223Z"
    }
   },
   "source": [
    "### Get working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-07T10:02:51.281914Z",
     "start_time": "2020-11-07T10:02:51.264960Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Admin'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-07T10:02:53.228689Z",
     "start_time": "2020-11-07T10:02:53.223702Z"
    }
   },
   "outputs": [],
   "source": [
    "os.chdir('C:\\\\Users\\\\Admin\\\\Desktop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-07T10:02:56.200239Z",
     "start_time": "2020-11-07T10:02:56.189268Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The First Line:  Narendra Modi Today on coronavirus LATEST Updates: Prime Minister Narendra Modi said that based on the suggestions by states, information about the extension of coronavirus lockdown will be given before 18 May. We will fight Corona and we will move forward, he added.\n",
      "\n",
      "The Last Line:  Separately, the Union health ministry observed that some relatively large outbreaks have been noticed in particular locations and it is important to focus on containment efforts to ensure that the country does not reach the community transmission stage.\n"
     ]
    }
   ],
   "source": [
    "## each entry of array will be a line\n",
    "\n",
    "file = open('corona.txt', mode = 'r', encoding = 'utf8')\n",
    "\n",
    "##Since Python 3.0, strings are stored as Unicode, i.e. each character in the string is represented by a code point. \n",
    "##So, each string is just a sequence of Unicode code points.\n",
    "## encoding - 'utf8'(uniform transformation format, 8 bit values are used in encoding)\n",
    "# mode = 'r' , open for reading in text mode\n",
    "\n",
    "lines = []\n",
    "for i in file:\n",
    "    lines.append(i)\n",
    "\n",
    "\n",
    "print(\"The First Line: \", lines[0])\n",
    "\n",
    "print(\"The Last Line: \", lines[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-07T10:02:58.100290Z",
     "start_time": "2020-11-07T10:02:58.089323Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Narendra Modi Today on coronavirus LATEST Updates: Prime Minister Narendra Modi said that based on the suggestions by states, information about the extension of coronavirus lockdown will be given before 18 May. We will fight Corona and we will move forward, he added.\\n',\n",
       " '\\n',\n",
       " 'We must not restrict our lives only around coronavirus , Modi said in his speech on Tuesday, and added that the fourth phase of the coronavirus lockdown in India will be completely different from the earlier three phases of the lockdown.\\n',\n",
       " '\\n',\n",
       " 'Saying that India has been an example for progress in the last century, Modi said that the country needs to become self-reliant in the world after the coronavirus pandemic.\\n',\n",
       " '\\n',\n",
       " '\"When the crisis started then not even a single PPE kit was manufactured in India, only a few N95 masks were available. Today two lakh PPE kits and 2 lakh N95 masks are manufactured in India daily,\" he added.\\n',\n",
       " '\\n',\n",
       " 'Prime minister Narendra Modi started speaking on COVID-19 situation, and said the world has been battling the pandemic for over four months now. Over 3 lakh people have succumbed to the infection, he says, condoling the deaths.\\n',\n",
       " '\\n',\n",
       " 'Prime Minister Narendra Modi on Tuesday approved ex-gratia of Rs 2 lakh each to the next of kin of 16 migrants who were run over by a goods train near Aurangabad in Maharashtra.\\n',\n",
       " '\\n',\n",
       " 'Modi is set to address the nation at 8 pm.\\n',\n",
       " '\\n',\n",
       " \"This will be the Modi's fifth address over the COVID-19 pandemic in the last two months. PM's address comes at a time when the country has over 70,000 cases.\\n\",\n",
       " '\\n',\n",
       " 'West Bengal chief minister Mamata Banerjee said that as relief from the COVID-19 pandemic is unlikely soon, there is a \"need for a three-month plan to deal with the situation\".\\n',\n",
       " '\\n',\n",
       " 'Mamata was also quoted as saying that people were facing problems since the imposition of the lockdown on 25 March as the move was \"poorly planned\", News18 reported.\\n',\n",
       " '\\n',\n",
       " 'The Maharashtra government on Tuesday allowed the home delivery of alcohol with guidelines. According to the order issued by the government, the delivery can be made \"only into the area of license, and people doing the delivery must wear mask and use sanitisers.\"\\n',\n",
       " '\\n',\n",
       " \"ANI quoted sources as saying that the second phase of the Centre's repatriation exercise, dubbed the 'Vande Bharat Mission', will be launched from 16-22 May. It will bring back Indians from 31 countries. 149 flights including feeder flights will be deployed, the report said.\\n\",\n",
       " '\\n',\n",
       " 'Health Minister Harsh Vardhan said that the mortality rate due to coronavirus is one of the lowest in India globally.\\n',\n",
       " '\\n',\n",
       " '\"In the fight against COVID-19 our mortality rate is about the lowest in the world. Today the mortality rate is around 3.2 percent, in several states it is even less than this. Global fatality rate is around 7-7.5 percent,\" he said.\\n',\n",
       " '\\n',\n",
       " 'Reports said that around 17,000 prisoners — almost 50 percent — are to be given parole in Maharashtra \"in order to maintain social distancing by decongesting in prisons\".\\n',\n",
       " '\\n',\n",
       " 'After 81 more individuals tested COVID-19 positive in Indore, the total number of confirmed cases in the Madhya Pradesh district climbed to 2,016, said a health official on Tuesday.\\n',\n",
       " '\\n',\n",
       " 'The COVID-19 toll in the district reached 92 after two more fatalities were registered in the past 24 hours.\\n',\n",
       " '\\n',\n",
       " 'As many as 6,037 Indian nationals have been evacuated in 31 inbound flights operated by Air India and Air India Express under Vande Bharat Mission, said Ministry of Civil Aviation on Tuesday.\\n',\n",
       " '\\n',\n",
       " 'The operation began five days ago from 7 May.\\n',\n",
       " '\\n',\n",
       " 'After nearly two months of hearing only \"extremely urgent matters\", the Supreme Court is likely to hear matters through video conferencing on a \"large scale\", reported Bar and Bench.\\n',\n",
       " '\\n',\n",
       " 'An observation to this effect was made by a three-judge Bench of the Supreme Court, while hearing a bail application.\\n',\n",
       " '\\n',\n",
       " 'The Bench of Justices L Nageshwara Rao, Abdul Nazeer and Sanjiv Khanna hinted at the same while hearing a bail application. It was observed that the matter would be taken up urgently when hearings begin on larger scale.\\n',\n",
       " '\\n',\n",
       " 'The current doubling rate of COVID-19 cases in Delhi stands now at 11 days, said health minister Satyendra Jain on Tuesday. \"The doubling rate in Delhi is 11 days now. The doubling rate had once reached 3 or 4 days. If the doubling rate reaches 18, 20 or 25, then we will be more comfortable,\" said Jain.\\n',\n",
       " '\\n',\n",
       " 'The COVID-19 recovery rate in the National Capital is now at 33 percent after 2,512 patients were cured.\\n',\n",
       " '\\n',\n",
       " '\"383 people have been cured/discharged taking the total number of recovered cases to 2512,\" he further said.\\n',\n",
       " '\\n',\n",
       " 'With 406 more people testing positive for the novel coronavirus in Delhi in the past 24 hours, the total number of confirmed cases in the National Capital climbed to 7,639, said health minister Satyendra Jain.\\n',\n",
       " '\\n',\n",
       " 'The COVID-19 toll in the Union Territory reached 86 after 13 more fatalities were reported since yesterday, said Jain.\\n',\n",
       " '\\n',\n",
       " 'Prime Minister Narendra Modi will address the nation on the COVID-19 situation at 8 pm on Tuesday, tweeted the Office of the Prime Minister of India.\\n',\n",
       " '\\n',\n",
       " 'Modi is likely to discuss about the possible extension of lockdown, imposed in view of coronavirus , for red zone districts.\\n',\n",
       " '\\n',\n",
       " 'Twenty-three people tested positive for COVID-19 in Odisha on Tuesday, taking the total number of confirmed cases in the state to 437, a health department official said.\\n',\n",
       " '\\n',\n",
       " 'With this, the number of active coronavirus cases stood at 349 in Odisha, while 85 people have recovered from the disease. Three persons have died of the infection in the state.\\n',\n",
       " '\\n',\n",
       " 'The Railway Ministry in a tweet said it is compulsory for all passengers to download the Aarogya Setu app before commencing their journey. Meanwhile, an Assistant Sub-inspector of CISF deployed in Kolkata lost his life last night due to COVID-19 infection on Tuesday.\\n',\n",
       " '\\n',\n",
       " 'As part of Samudra Setu Mission, Indian navy ship INS Jalashwa will sail again from Maldives capital of Male on Friday bringing back residents of Kerala and Lakshadweep. The ship had earlier brought back 698 Indians to Kochi on Sunday.\\n',\n",
       " '\\n',\n",
       " 'The ship, in its second ferry, was scheduled to bring people to Tuticorin but destination was changed since necessary approvals from Tamil Nadu government were not received.\\n',\n",
       " '\\n',\n",
       " 'With 47 more individuals testing positive for the novel coronavirus in Rajasthan, the total number of COVID-19 positive cases in the state climbed to 4,305, said the health department.\\n',\n",
       " '\\n',\n",
       " 'Two new COVID-19 deaths took the toll across the state to 151.\\n',\n",
       " '\\n',\n",
       " 'The total number of coronavirus cases in India climbed to 70,756 after more 3,604 individuals tested COVID-19 positive in the past 24 hours, said the health ministry on Tuesday. The COVID-19 toll reached 2,293 across the nation.\\n',\n",
       " '\\n',\n",
       " 'The figure includes 46,008 active cases, according to the data released by the Ministry of Health and Family Welfare.\\n',\n",
       " '\\n',\n",
       " 'The recovery rate stood at 31.7 percent after 22,454 COVID-19 patients were cured of the infectious disease.\\n',\n",
       " '\\n',\n",
       " 'With passenger train services set to begin from Tuesday, the Indian Railways on Monday said reservations were issued to more than 54,000 passengers within three hours.\\n',\n",
       " '\\n',\n",
       " 'Within minutes after booking started, all tickets across the three AC classes were sold out for Mumbai Central-New Delhi special train till 18 May, The Times of India reported.\\n',\n",
       " '\\n',\n",
       " 'The government released Rs 6,195.08 crore to 14 states as the second equated monthly installment of the Post Devolution Revenue Deficit Grant on Monday.\\n',\n",
       " '\\n',\n",
       " '\"This would provide them additional resources during the coronavirus crisis,\" the finance ministry said in a statement Monday.\\n',\n",
       " '\\n',\n",
       " 'The grant was recommended by the 15th Finance Commission, and an equal first installment of the grant was issued by the Centre to states on 14 March.\\n',\n",
       " '\\n',\n",
       " 'On Monday, the Centre released advance payments of over Rs 1,276 crore to Kerala, followed by over Rs 952 crore to Himachal Pradesh and over Rs 638 crore to Punjab. Assam received Rs 631 crore, Andhra Pradesh Rs 491 crore, Uttarakhand Rs 423 crore, and West Bengal got Rs 417 crore.\\n',\n",
       " '\\n',\n",
       " 'As India registered a record jump of 4,213 COVID-19 cases in the last 24 hours, Prime Minister Narendra Modi in a virtual interaction with chief ministers said that the biggest challenge for the country will be to ensure that the infection does not spread to rural India and that the country will have to devise a \"balanced strategy\" to deal with the pandemic and step up the economic activities in a gradual manner.\\n',\n",
       " '\\n',\n",
       " 'Separately, the Union health ministry observed that some relatively large outbreaks have been noticed in particular locations and it is important to focus on containment efforts to ensure that the country does not reach the community transmission stage.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-07T10:03:00.356329Z",
     "start_time": "2020-11-07T10:03:00.347351Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Narendra Modi Today on coronavirus LATEST Updates: Prime Minister Narendra Modi said that based on the suggestions by states, information about the extension of coronavirus lockdown will be given before 18 May. We will fight Corona and we will move forward, he added.\\n \\n We must not restrict our lives only around coronavirus , Modi said in his speech on Tuesday, and added that the fourth phase of the coronavirus lockdown in India will be completely different from the earlier three phases of the lockdown.\\n \\n Saying that India has been an example for progress in the last century, Modi said that the country needs to become self-reliant in the world after the coronavirus pandemic.\\n \\n \"When the crisis started then not even a single PPE kit was manufactured in India, only a few N95 masks were available. Today two lakh PPE kits and 2 lakh N95 masks are manufactured in India daily,\" he added.\\n \\n Prime minister Narendra Modi started speaking on COVID-19 situation, and said the world has been battling the pandemic for over four months now. Over 3 lakh people have succumbed to the infection, he says, condoling the deaths.\\n \\n Prime Minister Narendra Modi on Tuesday approved ex-gratia of Rs 2 lakh each to the next of kin of 16 migrants who were run over by a goods train near Aurangabad in Maharashtra.\\n \\n Modi is set to address the nation at 8 pm.\\n \\n This will be the Modi\\'s fifth address over the COVID-19 pandemic in the last two months. PM\\'s address comes at a time when the country has over 70,000 cases.\\n \\n West Bengal chief minister Mamata Banerjee said that as relief from the COVID-19 pandemic is unlikely soon, there is a \"need for a three-month plan to deal with the situation\".\\n \\n Mamata was also quoted as saying that people were facing problems since the imposition of the lockdown on 25 March as the move was \"poorly planned\", News18 reported.\\n \\n The Maharashtra government on Tuesday allowed the home delivery of alcohol with guidelines. According to the order issued by the government, the delivery can be made \"only into the area of license, and people doing the delivery must wear mask and use sanitisers.\"\\n \\n ANI quoted sources as saying that the second phase of the Centre\\'s repatriation exercise, dubbed the \\'Vande Bharat Mission\\', will be launched from 16-22 May. It will bring back Indians from 31 countries. 149 flights including feeder flights will be deployed, the report said.\\n \\n Health Minister Harsh Vardhan said that the mortality rate due to coronavirus is one of the lowest in India globally.\\n \\n \"In the fight against COVID-19 our mortality rate is about the lowest in the world. Today the mortality rate is around 3.2 percent, in several states it is even less than this. Global fatality rate is around 7-7.5 percent,\" he said.\\n \\n Reports said that around 17,000 prisoners — almost 50 percent — are to be given parole in Maharashtra \"in order to maintain social distancing by decongesting in prisons\".\\n \\n After 81 more individuals tested COVID-19 positive in Indore, the total number of confirmed cases in the Madhya Pradesh district climbed to 2,016, said a health official on Tuesday.\\n \\n The COVID-19 toll in the district reached 92 after two more fatalities were registered in the past 24 hours.\\n \\n As many as 6,037 Indian nationals have been evacuated in 31 inbound flights operated by Air India and Air India Express under Vande Bharat Mission, said Ministry of Civil Aviation on Tuesday.\\n \\n The operation began five days ago from 7 May.\\n \\n After nearly two months of hearing only \"extremely urgent matters\", the Supreme Court is likely to hear matters through video conferencing on a \"large scale\", reported Bar and Bench.\\n \\n An observation to this effect was made by a three-judge Bench of the Supreme Court, while hearing a bail application.\\n \\n The Bench of Justices L Nageshwara Rao, Abdul Nazeer and Sanjiv Khanna hinted at the same while hearing a bail application. It was observed that the matter would be taken up urgently when hearings begin on larger scale.\\n \\n The current doubling rate of COVID-19 cases in Delhi stands now at 11 days, said health minister Satyendra Jain on Tuesday. \"The doubling rate in Delhi is 11 days now. The doubling rate had once reached 3 or 4 days. If the doubling rate reaches 18, 20 or 25, then we will be more comfortable,\" said Jain.\\n \\n The COVID-19 recovery rate in the National Capital is now at 33 percent after 2,512 patients were cured.\\n \\n \"383 people have been cured/discharged taking the total number of recovered cases to 2512,\" he further said.\\n \\n With 406 more people testing positive for the novel coronavirus in Delhi in the past 24 hours, the total number of confirmed cases in the National Capital climbed to 7,639, said health minister Satyendra Jain.\\n \\n The COVID-19 toll in the Union Territory reached 86 after 13 more fatalities were reported since yesterday, said Jain.\\n \\n Prime Minister Narendra Modi will address the nation on the COVID-19 situation at 8 pm on Tuesday, tweeted the Office of the Prime Minister of India.\\n \\n Modi is likely to discuss about the possible extension of lockdown, imposed in view of coronavirus , for red zone districts.\\n \\n Twenty-three people tested positive for COVID-19 in Odisha on Tuesday, taking the total number of confirmed cases in the state to 437, a health department official said.\\n \\n With this, the number of active coronavirus cases stood at 349 in Odisha, while 85 people have recovered from the disease. Three persons have died of the infection in the state.\\n \\n The Railway Ministry in a tweet said it is compulsory for all passengers to download the Aarogya Setu app before commencing their journey. Meanwhile, an Assistant Sub-inspector of CISF deployed in Kolkata lost his life last night due to COVID-19 infection on Tuesday.\\n \\n As part of Samudra Setu Mission, Indian navy ship INS Jalashwa will sail again from Maldives capital of Male on Friday bringing back residents of Kerala and Lakshadweep. The ship had earlier brought back 698 Indians to Kochi on Sunday.\\n \\n The ship, in its second ferry, was scheduled to bring people to Tuticorin but destination was changed since necessary approvals from Tamil Nadu government were not received.\\n \\n With 47 more individuals testing positive for the novel coronavirus in Rajasthan, the total number of COVID-19 positive cases in the state climbed to 4,305, said the health department.\\n \\n Two new COVID-19 deaths took the toll across the state to 151.\\n \\n The total number of coronavirus cases in India climbed to 70,756 after more 3,604 individuals tested COVID-19 positive in the past 24 hours, said the health ministry on Tuesday. The COVID-19 toll reached 2,293 across the nation.\\n \\n The figure includes 46,008 active cases, according to the data released by the Ministry of Health and Family Welfare.\\n \\n The recovery rate stood at 31.7 percent after 22,454 COVID-19 patients were cured of the infectious disease.\\n \\n With passenger train services set to begin from Tuesday, the Indian Railways on Monday said reservations were issued to more than 54,000 passengers within three hours.\\n \\n Within minutes after booking started, all tickets across the three AC classes were sold out for Mumbai Central-New Delhi special train till 18 May, The Times of India reported.\\n \\n The government released Rs 6,195.08 crore to 14 states as the second equated monthly installment of the Post Devolution Revenue Deficit Grant on Monday.\\n \\n \"This would provide them additional resources during the coronavirus crisis,\" the finance ministry said in a statement Monday.\\n \\n The grant was recommended by the 15th Finance Commission, and an equal first installment of the grant was issued by the Centre to states on 14 March.\\n \\n On Monday, the Centre released advance payments of over Rs 1,276 crore to Kerala, followed by over Rs 952 crore to Himachal Pradesh and over Rs 638 crore to Punjab. Assam received Rs 631 crore, Andhra Pradesh Rs 491 crore, Uttarakhand Rs 423 crore, and West Bengal got Rs 417 crore.\\n \\n As India registered a record jump of 4,213 COVID-19 cases in the last 24 hours, Prime Minister Narendra Modi in a virtual interaction with chief ministers said that the biggest challenge for the country will be to ensure that the infection does not spread to rural India and that the country will have to devise a \"balanced strategy\" to deal with the pandemic and step up the economic activities in a gradual manner.\\n \\n Separately, the Union health ministry observed that some relatively large outbreaks have been noticed in particular locations and it is important to focus on containment efforts to ensure that the country does not reach the community transmission stage.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = \" \"\n",
    "\n",
    "## all lines combined\n",
    "for i in lines:\n",
    "    data = \" \".join(lines)\n",
    "    \n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-07T10:03:02.339094Z",
     "start_time": "2020-11-07T10:03:02.329122Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Narendra Modi Today on coronavirus LATEST Updates: Prime Minister Narendra Modi said that based on the suggestions by states, information about the extension of coronavirus lockdown will be given before 18 May. We will fight Corona and we will move forward, he added.    We must not restrict our lives only around coronavirus , Modi said in his speech on Tuesday, and added that the fourth phase of the coronavirus lockdown in India will be completely different from the earlier three phases of the lockdown.    Saying that India has been an example for progress in the last century, Modi said that the country needs to become self-reliant in the world after the coronavirus pandemic.    \"When the crisis started then not even a single PPE kit was manufactured in India, only a few N95 masks were available. Today two lakh PPE kits and 2 lakh N95 masks are manufactured in India daily,\" he added.    Prime minister Narendra Modi started speaking on COVID-19 situation, and said the world has been battling the pandemic for over four months now. Over 3 lakh people have succumbed to the infection, he says, condoling the deaths.    Prime Minister Narendra Modi on Tuesday approved ex-gratia of Rs 2 lakh each to the next of kin of 16 migrants who were run over by a goods train near Aurangabad in Maharashtra.    Modi is set to address the nation at 8 pm.    This will be the Modi\\'s fifth address over the COVID-19 pandemic in the last two months. PM\\'s address comes at a time when the country has over 70,000 cases.    West Bengal chief minister Mamata Banerjee said that as relief from the COVID-19 pandemic is unlikely soon, there is a \"need for a three-month plan to deal with the situation\".    Mamata was also quoted as saying that people were facing problems since the imposition of the lockdown on 25 March as the move was \"poorly planned\", News18 reported.    The Maharashtra government on Tuesday allowed the home delivery of alcohol with guidelines. According to the order issued by the government, the delivery can be made \"only into the area of license, and people doing the delivery must wear mask and use sanitisers.\"    ANI quoted sources as saying that the second phase of the Centre\\'s repatriation exercise, dubbed the \\'Vande Bharat Mission\\', will be launched from 16-22 May. It will bring back Indians from 31 countries. 149 flights including feeder flights will be deployed, the report said.    Health Minister Harsh Vardhan said that the mortality rate due to coronavirus is one of the lowest in India globally.    \"In the fight against COVID-19 our mortality rate is about the lowest in the world. Today the mortality rate is around 3.2 percent, in several states it is even less than this. Global fatality rate is around 7-7.5 percent,\" he said.    Reports said that around 17,000 prisoners — almost 50 percent — are to be given parole in Maharashtra \"in order to maintain social distancing by decongesting in prisons\".    After 81 more individuals tested COVID-19 positive in Indore, the total number of confirmed cases in the Madhya Pradesh district climbed to 2,016, said a health official on Tuesday.    The COVID-19 toll in the district reached 92 after two more fatalities were registered in the past 24 hours.    As many as 6,037 Indian nationals have been evacuated in 31 inbound flights operated by Air India and Air India Express under Vande Bharat Mission, said Ministry of Civil Aviation on Tuesday.    The operation began five days ago from 7 May.    After nearly two months of hearing only \"extremely urgent matters\", the Supreme Court is likely to hear matters through video conferencing on a \"large scale\", reported Bar and Bench.    An observation to this effect was made by a three-judge Bench of the Supreme Court, while hearing a bail application.    The Bench of Justices L Nageshwara Rao, Abdul Nazeer and Sanjiv Khanna hinted at the same while hearing a bail application. It was observed that the matter would be taken up urgently when hearings begin on larger scale.    The current doubling rate of COVID-19 cases in Delhi stands now at 11 days, said health minister Satyendra Jain on Tuesday. \"The doubling rate in Delhi is 11 days now. The doubling rate had once reached 3 or 4 days. If the doubling rate reaches 18, 20 or 25, then we will be more comfortable,\" said Jain.    The COVID-19 recovery rate in the National Capital is now at 33 percent after 2,512 patients were cured.    \"383 people have been cured/discharged taking the total number of recovered cases to 2512,\" he further said.    With 406 more people testing positive for the novel coronavirus in Delhi in the past 24 hours, the total number of confirmed cases in the National Capital climbed to 7,639, said health minister Satyendra Jain.    The COVID-19 toll in the Union Territory reached 86 after 13 more fatalities were reported since yesterday, said Jain.    Prime Minister Narendra Modi will address the nation on the COVID-19 situation at 8 pm on Tuesday, tweeted the Office of the Prime Minister of India.    Modi is likely to discuss about the possible extension of lockdown, imposed in view of coronavirus , for red zone districts.    Twenty-three people tested positive for COVID-19 in Odisha on Tuesday, taking the total number of confirmed cases in the state to 437, a health department official said.    With this, the number of active coronavirus cases stood at 349 in Odisha, while 85 people have recovered from the disease. Three persons have died of the infection in the state.    The Railway Ministry in a tweet said it is compulsory for all passengers to download the Aarogya Setu app before commencing their journey. Meanwhile, an Assistant Sub-inspector of CISF deployed in Kolkata lost his life last night due to COVID-19 infection on Tuesday.    As part of Samudra Setu Mission, Indian navy ship INS Jalashwa will sail again from Maldives capital of Male on Friday bringing back residents of Kerala and Lakshadweep. The ship had earlier brought back 698 Indians to Kochi on Sunday.    The ship, in its second ferry, was scheduled to bring people to Tuticorin but destination was changed since necessary approvals from Tamil Nadu government were not received.    With 47 more individuals testing positive for the novel coronavirus in Rajasthan, the total number of COVID-19 positive cases in the state climbed to 4,305, said the health department.    Two new COVID-19 deaths took the toll across the state to 151.    The total number of coronavirus cases in India climbed to 70,756 after more 3,604 individuals tested COVID-19 positive in the past 24 hours, said the health ministry on Tuesday. The COVID-19 toll reached 2,293 across the nation.    The figure includes 46,008 active cases, according to the data released by the Ministry of Health and Family Welfare.    The recovery rate stood at 31.7 percent after 22,454 COVID-19 patients were cured of the infectious disease.    With passenger train services set to begin from Tuesday, the Indian Railways on Monday said reservations were issued to more than 54,000 passengers within three hours.    Within minutes after booking started, all tickets across the three AC classes were sold out for Mumbai Central-New Delhi special train till 18 May, The Times of India reported.    The government released Rs 6,195.08 crore to 14 states as the second equated monthly installment of the Post Devolution Revenue Deficit Grant on Monday.    \"This would provide them additional resources during the coronavirus crisis,\" the finance ministry said in a statement Monday.    The grant was recommended by the 15th Finance Commission, and an equal first installment of the grant was issued by the Centre to states on 14 March.    On Monday, the Centre released advance payments of over Rs 1,276 crore to Kerala, followed by over Rs 952 crore to Himachal Pradesh and over Rs 638 crore to Punjab. Assam received Rs 631 crore, Andhra Pradesh Rs 491 crore, Uttarakhand Rs 423 crore, and West Bengal got Rs 417 crore.    As India registered a record jump of 4,213 COVID-19 cases in the last 24 hours, Prime Minister Narendra Modi in a virtual interaction with chief ministers said that the biggest challenge for the country will be to ensure that the infection does not spread to rural India and that the country will have to devise a \"balanced strategy\" to deal with the pandemic and step up the economic activities in a gradual manner.    Separately, the Union health ministry observed that some relatively large outbreaks have been noticed in particular locations and it is important to focus on containment efforts to ensure that the country does not reach the community transmission stage.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## replace bad words\n",
    "\n",
    "data = data.replace('\\n',' ').replace('\\r', ' ').replace('\\t', ' ').replace('ufeff', ' ')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-07T10:03:10.033323Z",
     "start_time": "2020-11-07T10:03:10.020356Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Narendra Modi Today on coronavirus LATEST Updates  Prime Minister Narendra Modi said that based on the suggestions by states  information about the extension of coronavirus lockdown will be given before 18 May  We will fight Corona and we will move forward  he added     We must not restrict our lives only around coronavirus   Modi said in his speech on Tuesday  and added that the fourth phase of the coronavirus lockdown in India will be completely different from the earlier three phases of the lockdown     Saying that India has been an example for progress in the last century  Modi said that the country needs to become self reliant in the world after the coronavirus pandemic      When the crisis started then not even a single PPE kit was manufactured in India  only a few N95 masks were available  Today two lakh PPE kits and 2 lakh N95 masks are manufactured in India daily   he added     Prime minister Narendra Modi started speaking on COVID 19 situation  and said the world has been battling the pandemic for over four months now  Over 3 lakh people have succumbed to the infection  he says  condoling the deaths     Prime Minister Narendra Modi on Tuesday approved ex gratia of Rs 2 lakh each to the next of kin of 16 migrants who were run over by a goods train near Aurangabad in Maharashtra     Modi is set to address the nation at 8 pm     This will be the Modi s fifth address over the COVID 19 pandemic in the last two months  PM s address comes at a time when the country has over 70 000 cases     West Bengal chief minister Mamata Banerjee said that as relief from the COVID 19 pandemic is unlikely soon  there is a  need for a three month plan to deal with the situation      Mamata was also quoted as saying that people were facing problems since the imposition of the lockdown on 25 March as the move was  poorly planned   News18 reported     The Maharashtra government on Tuesday allowed the home delivery of alcohol with guidelines  According to the order issued by the government  the delivery can be made  only into the area of license  and people doing the delivery must wear mask and use sanitisers      ANI quoted sources as saying that the second phase of the Centre s repatriation exercise  dubbed the  Vande Bharat Mission   will be launched from 16 22 May  It will bring back Indians from 31 countries  149 flights including feeder flights will be deployed  the report said     Health Minister Harsh Vardhan said that the mortality rate due to coronavirus is one of the lowest in India globally      In the fight against COVID 19 our mortality rate is about the lowest in the world  Today the mortality rate is around 3 2 percent  in several states it is even less than this  Global fatality rate is around 7 7 5 percent   he said     Reports said that around 17 000 prisoners — almost 50 percent — are to be given parole in Maharashtra  in order to maintain social distancing by decongesting in prisons      After 81 more individuals tested COVID 19 positive in Indore  the total number of confirmed cases in the Madhya Pradesh district climbed to 2 016  said a health official on Tuesday     The COVID 19 toll in the district reached 92 after two more fatalities were registered in the past 24 hours     As many as 6 037 Indian nationals have been evacuated in 31 inbound flights operated by Air India and Air India Express under Vande Bharat Mission  said Ministry of Civil Aviation on Tuesday     The operation began five days ago from 7 May     After nearly two months of hearing only  extremely urgent matters   the Supreme Court is likely to hear matters through video conferencing on a  large scale   reported Bar and Bench     An observation to this effect was made by a three judge Bench of the Supreme Court  while hearing a bail application     The Bench of Justices L Nageshwara Rao  Abdul Nazeer and Sanjiv Khanna hinted at the same while hearing a bail application  It was observed that the matter would be taken up urgently when hearings begin on larger scale     The current doubling rate of COVID 19 cases in Delhi stands now at 11 days  said health minister Satyendra Jain on Tuesday   The doubling rate in Delhi is 11 days now  The doubling rate had once reached 3 or 4 days  If the doubling rate reaches 18  20 or 25  then we will be more comfortable   said Jain     The COVID 19 recovery rate in the National Capital is now at 33 percent after 2 512 patients were cured      383 people have been cured discharged taking the total number of recovered cases to 2512   he further said     With 406 more people testing positive for the novel coronavirus in Delhi in the past 24 hours  the total number of confirmed cases in the National Capital climbed to 7 639  said health minister Satyendra Jain     The COVID 19 toll in the Union Territory reached 86 after 13 more fatalities were reported since yesterday  said Jain     Prime Minister Narendra Modi will address the nation on the COVID 19 situation at 8 pm on Tuesday  tweeted the Office of the Prime Minister of India     Modi is likely to discuss about the possible extension of lockdown  imposed in view of coronavirus   for red zone districts     Twenty three people tested positive for COVID 19 in Odisha on Tuesday  taking the total number of confirmed cases in the state to 437  a health department official said     With this  the number of active coronavirus cases stood at 349 in Odisha  while 85 people have recovered from the disease  Three persons have died of the infection in the state     The Railway Ministry in a tweet said it is compulsory for all passengers to download the Aarogya Setu app before commencing their journey  Meanwhile  an Assistant Sub inspector of CISF deployed in Kolkata lost his life last night due to COVID 19 infection on Tuesday     As part of Samudra Setu Mission  Indian navy ship INS Jalashwa will sail again from Maldives capital of Male on Friday bringing back residents of Kerala and Lakshadweep  The ship had earlier brought back 698 Indians to Kochi on Sunday     The ship  in its second ferry  was scheduled to bring people to Tuticorin but destination was changed since necessary approvals from Tamil Nadu government were not received     With 47 more individuals testing positive for the novel coronavirus in Rajasthan  the total number of COVID 19 positive cases in the state climbed to 4 305  said the health department     Two new COVID 19 deaths took the toll across the state to 151     The total number of coronavirus cases in India climbed to 70 756 after more 3 604 individuals tested COVID 19 positive in the past 24 hours  said the health ministry on Tuesday  The COVID 19 toll reached 2 293 across the nation     The figure includes 46 008 active cases  according to the data released by the Ministry of Health and Family Welfare     The recovery rate stood at 31 7 percent after 22 454 COVID 19 patients were cured of the infectious disease     With passenger train services set to begin from Tuesday  the Indian Railways on Monday said reservations were issued to more than 54 000 passengers within three hours     Within minutes after booking started  all tickets across the three AC classes were sold out for Mumbai Central New Delhi special train till 18 May  The Times of India reported     The government released Rs 6 195 08 crore to 14 states as the second equated monthly installment of the Post Devolution Revenue Deficit Grant on Monday      This would provide them additional resources during the coronavirus crisis   the finance ministry said in a statement Monday     The grant was recommended by the 15th Finance Commission  and an equal first installment of the grant was issued by the Centre to states on 14 March     On Monday  the Centre released advance payments of over Rs 1 276 crore to Kerala  followed by over Rs 952 crore to Himachal Pradesh and over Rs 638 crore to Punjab  Assam received Rs 631 crore  Andhra Pradesh Rs 491 crore  Uttarakhand Rs 423 crore  and West Bengal got Rs 417 crore     As India registered a record jump of 4 213 COVID 19 cases in the last 24 hours  Prime Minister Narendra Modi in a virtual interaction with chief ministers said that the biggest challenge for the country will be to ensure that the infection does not spread to rural India and that the country will have to devise a  balanced strategy  to deal with the pandemic and step up the economic activities in a gradual manner     Separately  the Union health ministry observed that some relatively large outbreaks have been noticed in particular locations and it is important to focus on containment efforts to ensure that the country does not reach the community transmission stage '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## to remove punctuations with spaces\n",
    "import string\n",
    "\n",
    "## creating an obj called - translator\n",
    "\n",
    "translator = str.maketrans(string.punctuation,\" \"*len(string.punctuation)) ## mapping punctuation with space\n",
    "\n",
    "new_data = data.translate(translator)\n",
    "\n",
    "new_data\n",
    "\n",
    "## words acting as x and y\n",
    "# Narendra - x\n",
    "# Modi - y\n",
    "#then Modi is x to predict next word i.e, y (Today)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-07T10:03:14.333833Z",
     "start_time": "2020-11-07T10:03:14.319869Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Narendra Modi Today on coronavirus LATEST Updates Prime Minister said that based the suggestions by states information about extension of lockdown will be given before 18 May We fight Corona and we move forward he added must not restrict our lives only around in his speech Tuesday fourth phase India completely different from earlier three phases Saying has been an example for progress last century country needs to become self reliant world after pandemic When crisis started then even a single PPE kit was manufactured few N95 masks were available two lakh kits 2 are daily minister speaking COVID 19 situation battling over four months now Over 3 people have succumbed infection says condoling deaths approved ex gratia Rs each next kin 16 migrants who run goods train near Aurangabad Maharashtra is set address nation at 8 pm This s fifth PM comes time when 70 000 cases West Bengal chief Mamata Banerjee as relief unlikely soon there need month plan deal with also quoted saying facing problems since imposition 25 March poorly planned News18 reported The government allowed home delivery alcohol guidelines According order issued can made into area license doing wear mask use sanitisers ANI sources second Centre repatriation exercise dubbed Vande Bharat Mission launched 22 It bring back Indians 31 countries 149 flights including feeder deployed report Health Harsh Vardhan mortality rate due one lowest globally In against percent several it less than this Global fatality 7 5 Reports 17 prisoners — almost 50 parole maintain social distancing decongesting prisons After 81 more individuals tested positive Indore total number confirmed Madhya Pradesh district climbed 016 health official toll reached 92 fatalities registered past 24 hours As many 6 037 Indian nationals evacuated inbound operated Air Express under Ministry Civil Aviation operation began five days ago nearly hearing extremely urgent matters Supreme Court likely hear through video conferencing large scale Bar Bench An observation effect judge while bail application Justices L Nageshwara Rao Abdul Nazeer Sanjiv Khanna hinted same observed matter would taken up urgently hearings begin larger current doubling Delhi stands 11 Satyendra Jain had once or 4 If reaches 20 comfortable recovery National Capital 33 512 patients cured 383 discharged taking recovered 2512 further With 406 testing novel 639 Union Territory 86 13 yesterday tweeted Office discuss possible imposed view red zone districts Twenty Odisha state 437 department active stood 349 85 disease Three persons died Railway tweet compulsory all passengers download Aarogya Setu app commencing their journey Meanwhile Assistant Sub inspector CISF Kolkata lost life night part Samudra navy ship INS Jalashwa sail again Maldives capital Male Friday bringing residents Kerala Lakshadweep brought 698 Kochi Sunday its ferry scheduled Tuticorin but destination changed necessary approvals Tamil Nadu received 47 Rajasthan 305 Two new took across 151 756 604 ministry 293 figure includes 46 008 according data released Family Welfare 454 infectious passenger services Railways Monday reservations 54 within Within minutes booking tickets AC classes sold out Mumbai Central New special till Times 195 08 crore 14 equated monthly installment Post Devolution Revenue Deficit Grant provide them additional resources during finance statement grant recommended 15th Finance Commission equal first On advance payments 1 276 followed 952 Himachal 638 Punjab Assam 631 Andhra 491 Uttarakhand 423 got 417 record jump 213 virtual interaction ministers biggest challenge ensure does spread rural devise balanced strategy step economic activities gradual manner Separately some relatively outbreaks noticed particular locations important focus containment efforts reach community transmission stage'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Out of the given data, we need to create a dictionary of unique words\\\\\n",
    "\n",
    "z = []\n",
    "\n",
    "for i in new_data.split():\n",
    "    if i not in z:\n",
    "        z.append(i)\n",
    "        \n",
    "z\n",
    "\n",
    "## combining all the words in a line\n",
    "\n",
    "new_data = ' '.join(z)\n",
    "\n",
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-07T10:03:16.199161Z",
     "start_time": "2020-11-07T10:03:16.192179Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3824"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## checking the length of the data\n",
    "\n",
    "len(new_data)\n",
    "\n",
    "## 3824"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-07T10:04:08.101386Z",
     "start_time": "2020-11-07T10:04:08.067476Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[26,\n",
       " 27,\n",
       " 28,\n",
       " 1,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 2,\n",
       " 26,\n",
       " 27,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 1,\n",
       " 3,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 3,\n",
       " 41,\n",
       " 42,\n",
       " 29,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 4,\n",
       " 44,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 4,\n",
       " 44,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 4,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 29,\n",
       " 27,\n",
       " 33,\n",
       " 5,\n",
       " 64,\n",
       " 65,\n",
       " 1,\n",
       " 66,\n",
       " 52,\n",
       " 56,\n",
       " 34,\n",
       " 3,\n",
       " 67,\n",
       " 68,\n",
       " 42,\n",
       " 3,\n",
       " 29,\n",
       " 43,\n",
       " 5,\n",
       " 69,\n",
       " 44,\n",
       " 45,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 3,\n",
       " 73,\n",
       " 6,\n",
       " 74,\n",
       " 42,\n",
       " 3,\n",
       " 43,\n",
       " 7,\n",
       " 34,\n",
       " 69,\n",
       " 75,\n",
       " 76,\n",
       " 8,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 5,\n",
       " 3,\n",
       " 80,\n",
       " 81,\n",
       " 27,\n",
       " 33,\n",
       " 34,\n",
       " 3,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 5,\n",
       " 3,\n",
       " 88,\n",
       " 9,\n",
       " 3,\n",
       " 29,\n",
       " 89,\n",
       " 10,\n",
       " 3,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 58,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 5,\n",
       " 69,\n",
       " 62,\n",
       " 94,\n",
       " 100,\n",
       " 101,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 28,\n",
       " 11,\n",
       " 105,\n",
       " 96,\n",
       " 106,\n",
       " 52,\n",
       " 107,\n",
       " 105,\n",
       " 101,\n",
       " 102,\n",
       " 108,\n",
       " 99,\n",
       " 5,\n",
       " 69,\n",
       " 109,\n",
       " 55,\n",
       " 56,\n",
       " 32,\n",
       " 2,\n",
       " 26,\n",
       " 27,\n",
       " 91,\n",
       " 110,\n",
       " 1,\n",
       " 111,\n",
       " 112,\n",
       " 113,\n",
       " 52,\n",
       " 33,\n",
       " 3,\n",
       " 88,\n",
       " 75,\n",
       " 76,\n",
       " 114,\n",
       " 3,\n",
       " 89,\n",
       " 78,\n",
       " 12,\n",
       " 115,\n",
       " 116,\n",
       " 117,\n",
       " 12,\n",
       " 118,\n",
       " 105,\n",
       " 119,\n",
       " 120,\n",
       " 121,\n",
       " 84,\n",
       " 3,\n",
       " 122,\n",
       " 55,\n",
       " 123,\n",
       " 124,\n",
       " 3,\n",
       " 125,\n",
       " 32,\n",
       " 2,\n",
       " 26,\n",
       " 27,\n",
       " 1,\n",
       " 66,\n",
       " 126,\n",
       " 127,\n",
       " 128,\n",
       " 42,\n",
       " 129,\n",
       " 107,\n",
       " 105,\n",
       " 130,\n",
       " 84,\n",
       " 3,\n",
       " 131,\n",
       " 42,\n",
       " 132,\n",
       " 42,\n",
       " 133,\n",
       " 134,\n",
       " 135,\n",
       " 103,\n",
       " 136,\n",
       " 12,\n",
       " 37,\n",
       " 94,\n",
       " 137,\n",
       " 138,\n",
       " 139,\n",
       " 140,\n",
       " 5,\n",
       " 141,\n",
       " 27,\n",
       " 142,\n",
       " 143,\n",
       " 84,\n",
       " 144,\n",
       " 3,\n",
       " 145,\n",
       " 146,\n",
       " 147,\n",
       " 13,\n",
       " 14,\n",
       " 44,\n",
       " 45,\n",
       " 3,\n",
       " 149,\n",
       " 144,\n",
       " 12,\n",
       " 3,\n",
       " 111,\n",
       " 112,\n",
       " 89,\n",
       " 5,\n",
       " 3,\n",
       " 80,\n",
       " 11,\n",
       " 116,\n",
       " 144,\n",
       " 150,\n",
       " 146,\n",
       " 94,\n",
       " 151,\n",
       " 10,\n",
       " 3,\n",
       " 82,\n",
       " 75,\n",
       " 12,\n",
       " 152,\n",
       " 153,\n",
       " 154,\n",
       " 155,\n",
       " 156,\n",
       " 157,\n",
       " 2,\n",
       " 158,\n",
       " 159,\n",
       " 33,\n",
       " 34,\n",
       " 15,\n",
       " 160,\n",
       " 72,\n",
       " 3,\n",
       " 111,\n",
       " 112,\n",
       " 89,\n",
       " 142,\n",
       " 161,\n",
       " 162,\n",
       " 163,\n",
       " 142,\n",
       " 94,\n",
       " 164,\n",
       " 78,\n",
       " 94,\n",
       " 6,\n",
       " 165,\n",
       " 166,\n",
       " 84,\n",
       " 167,\n",
       " 16,\n",
       " 3,\n",
       " 113,\n",
       " 158,\n",
       " 98,\n",
       " 168,\n",
       " 169,\n",
       " 15,\n",
       " 7,\n",
       " 34,\n",
       " 119,\n",
       " 103,\n",
       " 170,\n",
       " 171,\n",
       " 172,\n",
       " 3,\n",
       " 173,\n",
       " 42,\n",
       " 3,\n",
       " 43,\n",
       " 1,\n",
       " 174,\n",
       " 175,\n",
       " 15,\n",
       " 3,\n",
       " 53,\n",
       " 98,\n",
       " 176,\n",
       " 177,\n",
       " 178,\n",
       " 179,\n",
       " 3,\n",
       " 141,\n",
       " 180,\n",
       " 1,\n",
       " 66,\n",
       " 181,\n",
       " 3,\n",
       " 182,\n",
       " 183,\n",
       " 42,\n",
       " 184,\n",
       " 16,\n",
       " 185,\n",
       " 17,\n",
       " 84,\n",
       " 3,\n",
       " 186,\n",
       " 187,\n",
       " 37,\n",
       " 3,\n",
       " 180,\n",
       " 3,\n",
       " 183,\n",
       " 188,\n",
       " 45,\n",
       " 189,\n",
       " 62,\n",
       " 190,\n",
       " 3,\n",
       " 191,\n",
       " 42,\n",
       " 192,\n",
       " 52,\n",
       " 119,\n",
       " 193,\n",
       " 3,\n",
       " 183,\n",
       " 57,\n",
       " 194,\n",
       " 195,\n",
       " 52,\n",
       " 196,\n",
       " 197,\n",
       " 198,\n",
       " 169,\n",
       " 199,\n",
       " 15,\n",
       " 7,\n",
       " 34,\n",
       " 3,\n",
       " 200,\n",
       " 68,\n",
       " 42,\n",
       " 3,\n",
       " 202,\n",
       " 203,\n",
       " 204,\n",
       " 3,\n",
       " 206,\n",
       " 44,\n",
       " 45,\n",
       " 208,\n",
       " 72,\n",
       " 133,\n",
       " 209,\n",
       " 49,\n",
       " 18,\n",
       " 44,\n",
       " 210,\n",
       " 211,\n",
       " 212,\n",
       " 72,\n",
       " 213,\n",
       " 214,\n",
       " 215,\n",
       " 216,\n",
       " 217,\n",
       " 218,\n",
       " 216,\n",
       " 44,\n",
       " 45,\n",
       " 219,\n",
       " 3,\n",
       " 220,\n",
       " 33,\n",
       " 19,\n",
       " 2,\n",
       " 221,\n",
       " 222,\n",
       " 33,\n",
       " 34,\n",
       " 3,\n",
       " 223,\n",
       " 224,\n",
       " 225,\n",
       " 84,\n",
       " 29,\n",
       " 142,\n",
       " 226,\n",
       " 42,\n",
       " 3,\n",
       " 227,\n",
       " 5,\n",
       " 69,\n",
       " 228,\n",
       " 5,\n",
       " 3,\n",
       " 50,\n",
       " 229,\n",
       " 111,\n",
       " 112,\n",
       " 60,\n",
       " 223,\n",
       " 224,\n",
       " 142,\n",
       " 40,\n",
       " 3,\n",
       " 227,\n",
       " 5,\n",
       " 3,\n",
       " 88,\n",
       " 28,\n",
       " 3,\n",
       " 223,\n",
       " 224,\n",
       " 142,\n",
       " 63,\n",
       " 118,\n",
       " 107,\n",
       " 230,\n",
       " 5,\n",
       " 231,\n",
       " 38,\n",
       " 18,\n",
       " 142,\n",
       " 93,\n",
       " 232,\n",
       " 233,\n",
       " 14,\n",
       " 234,\n",
       " 235,\n",
       " 224,\n",
       " 142,\n",
       " 63,\n",
       " 236,\n",
       " 236,\n",
       " 237,\n",
       " 230,\n",
       " 55,\n",
       " 33,\n",
       " 238,\n",
       " 33,\n",
       " 34,\n",
       " 63,\n",
       " 239,\n",
       " 153,\n",
       " 240,\n",
       " 241,\n",
       " 242,\n",
       " 243,\n",
       " 230,\n",
       " 241,\n",
       " 108,\n",
       " 84,\n",
       " 45,\n",
       " 46,\n",
       " 244,\n",
       " 5,\n",
       " 141,\n",
       " 5,\n",
       " 186,\n",
       " 84,\n",
       " 245,\n",
       " 246,\n",
       " 247,\n",
       " 37,\n",
       " 248,\n",
       " 5,\n",
       " 249,\n",
       " 9,\n",
       " 250,\n",
       " 251,\n",
       " 252,\n",
       " 253,\n",
       " 111,\n",
       " 112,\n",
       " 254,\n",
       " 5,\n",
       " 255,\n",
       " 3,\n",
       " 256,\n",
       " 257,\n",
       " 42,\n",
       " 258,\n",
       " 154,\n",
       " 5,\n",
       " 3,\n",
       " 259,\n",
       " 260,\n",
       " 261,\n",
       " 262,\n",
       " 84,\n",
       " 107,\n",
       " 263,\n",
       " 33,\n",
       " 94,\n",
       " 19,\n",
       " 264,\n",
       " 1,\n",
       " 66,\n",
       " 3,\n",
       " 111,\n",
       " 112,\n",
       " 265,\n",
       " 5,\n",
       " 3,\n",
       " 261,\n",
       " 266,\n",
       " 267,\n",
       " 9,\n",
       " 11,\n",
       " 251,\n",
       " 268,\n",
       " 103,\n",
       " 269,\n",
       " 5,\n",
       " 3,\n",
       " 270,\n",
       " 271,\n",
       " 272,\n",
       " 15,\n",
       " 273,\n",
       " 15,\n",
       " 274,\n",
       " 275,\n",
       " 276,\n",
       " 277,\n",
       " 120,\n",
       " 76,\n",
       " 278,\n",
       " 5,\n",
       " 213,\n",
       " 279,\n",
       " 216,\n",
       " 280,\n",
       " 37,\n",
       " 281,\n",
       " 69,\n",
       " 52,\n",
       " 281,\n",
       " 69,\n",
       " 282,\n",
       " 283,\n",
       " 205,\n",
       " 206,\n",
       " 207,\n",
       " 33,\n",
       " 20,\n",
       " 42,\n",
       " 284,\n",
       " 285,\n",
       " 1,\n",
       " 66,\n",
       " 3,\n",
       " 286,\n",
       " 287,\n",
       " 288,\n",
       " 289,\n",
       " 290,\n",
       " 72,\n",
       " 236,\n",
       " 49,\n",
       " 9,\n",
       " 291,\n",
       " 11,\n",
       " 116,\n",
       " 42,\n",
       " 292,\n",
       " 62,\n",
       " 293,\n",
       " 294,\n",
       " 295,\n",
       " 3,\n",
       " 296,\n",
       " 297,\n",
       " 142,\n",
       " 298,\n",
       " 84,\n",
       " 299,\n",
       " 295,\n",
       " 300,\n",
       " 301,\n",
       " 302,\n",
       " 1,\n",
       " 94,\n",
       " 303,\n",
       " 304,\n",
       " 179,\n",
       " 305,\n",
       " 52,\n",
       " 306,\n",
       " 8,\n",
       " 307,\n",
       " 84,\n",
       " 14,\n",
       " 308,\n",
       " 98,\n",
       " 189,\n",
       " 37,\n",
       " 94,\n",
       " 6,\n",
       " 309,\n",
       " 306,\n",
       " 42,\n",
       " 3,\n",
       " 296,\n",
       " 297,\n",
       " 310,\n",
       " 292,\n",
       " 94,\n",
       " 311,\n",
       " 312,\n",
       " 3,\n",
       " 306,\n",
       " 42,\n",
       " 313,\n",
       " 314,\n",
       " 315,\n",
       " 316,\n",
       " 317,\n",
       " 318,\n",
       " 52,\n",
       " 319,\n",
       " 320,\n",
       " 321,\n",
       " 146,\n",
       " 3,\n",
       " 322,\n",
       " 310,\n",
       " 292,\n",
       " 94,\n",
       " 311,\n",
       " 312,\n",
       " 18,\n",
       " 98,\n",
       " 323,\n",
       " 34,\n",
       " 3,\n",
       " 324,\n",
       " 325,\n",
       " 45,\n",
       " 326,\n",
       " 327,\n",
       " 328,\n",
       " 10,\n",
       " 329,\n",
       " 330,\n",
       " 1,\n",
       " 331,\n",
       " 304,\n",
       " 3,\n",
       " 332,\n",
       " 333,\n",
       " 224,\n",
       " 42,\n",
       " 111,\n",
       " 112,\n",
       " 154,\n",
       " 5,\n",
       " 334,\n",
       " 335,\n",
       " 117,\n",
       " 146,\n",
       " 336,\n",
       " 289,\n",
       " 33,\n",
       " 19,\n",
       " 2,\n",
       " 337,\n",
       " 338,\n",
       " 1,\n",
       " 66,\n",
       " 3,\n",
       " 333,\n",
       " 224,\n",
       " 5,\n",
       " 334,\n",
       " 142,\n",
       " 336,\n",
       " 289,\n",
       " 117,\n",
       " 3,\n",
       " 333,\n",
       " 224,\n",
       " 339,\n",
       " 340,\n",
       " 266,\n",
       " 118,\n",
       " 341,\n",
       " 342,\n",
       " 289,\n",
       " 343,\n",
       " 3,\n",
       " 333,\n",
       " 224,\n",
       " 344,\n",
       " 48,\n",
       " 345,\n",
       " 341,\n",
       " 174,\n",
       " 92,\n",
       " 4,\n",
       " 44,\n",
       " 45,\n",
       " 251,\n",
       " 346,\n",
       " 33,\n",
       " 338,\n",
       " 3,\n",
       " 111,\n",
       " 112,\n",
       " 347,\n",
       " 224,\n",
       " 5,\n",
       " 3,\n",
       " 348,\n",
       " 21,\n",
       " 142,\n",
       " 117,\n",
       " 146,\n",
       " 349,\n",
       " 230,\n",
       " 9,\n",
       " 107,\n",
       " 350,\n",
       " 351,\n",
       " 103,\n",
       " 352,\n",
       " 353,\n",
       " 119,\n",
       " 120,\n",
       " 76,\n",
       " 352,\n",
       " 354,\n",
       " 355,\n",
       " 3,\n",
       " 256,\n",
       " 257,\n",
       " 42,\n",
       " 356,\n",
       " 154,\n",
       " 84,\n",
       " 357,\n",
       " 55,\n",
       " 358,\n",
       " 33,\n",
       " 16,\n",
       " 359,\n",
       " 251,\n",
       " 119,\n",
       " 360,\n",
       " 254,\n",
       " 78,\n",
       " 3,\n",
       " 361,\n",
       " 29,\n",
       " 5,\n",
       " 334,\n",
       " 5,\n",
       " 3,\n",
       " 270,\n",
       " 271,\n",
       " 272,\n",
       " 3,\n",
       " 256,\n",
       " 257,\n",
       " 42,\n",
       " 258,\n",
       " 154,\n",
       " 5,\n",
       " 3,\n",
       " 348,\n",
       " 21,\n",
       " 262,\n",
       " 84,\n",
       " 236,\n",
       " 362,\n",
       " 33,\n",
       " 19,\n",
       " 2,\n",
       " 337,\n",
       " 338,\n",
       " 3,\n",
       " 111,\n",
       " 112,\n",
       " 265,\n",
       " 5,\n",
       " 3,\n",
       " 363,\n",
       " 364,\n",
       " 266,\n",
       " 365,\n",
       " 9,\n",
       " 366,\n",
       " 251,\n",
       " 268,\n",
       " 103,\n",
       " 179,\n",
       " 172,\n",
       " 367,\n",
       " 33,\n",
       " 338,\n",
       " 32,\n",
       " 2,\n",
       " 26,\n",
       " 27,\n",
       " 44,\n",
       " 144,\n",
       " 3,\n",
       " 145,\n",
       " 1,\n",
       " 3,\n",
       " 111,\n",
       " 112,\n",
       " 113,\n",
       " 146,\n",
       " 147,\n",
       " 13,\n",
       " 1,\n",
       " 66,\n",
       " 368,\n",
       " 3,\n",
       " 369,\n",
       " 42,\n",
       " 3,\n",
       " 32,\n",
       " 2,\n",
       " 42,\n",
       " 69,\n",
       " 27,\n",
       " 142,\n",
       " 298,\n",
       " 84,\n",
       " 370,\n",
       " 40,\n",
       " 3,\n",
       " 371,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 372,\n",
       " 5,\n",
       " 373,\n",
       " 42,\n",
       " 29,\n",
       " 78,\n",
       " 374,\n",
       " 375,\n",
       " 376,\n",
       " 377,\n",
       " 6,\n",
       " 119,\n",
       " 253,\n",
       " 254,\n",
       " 78,\n",
       " 111,\n",
       " 112,\n",
       " 5,\n",
       " 378,\n",
       " 1,\n",
       " 66,\n",
       " 355,\n",
       " 3,\n",
       " 256,\n",
       " 257,\n",
       " 42,\n",
       " 258,\n",
       " 154,\n",
       " 5,\n",
       " 3,\n",
       " 379,\n",
       " 84,\n",
       " 380,\n",
       " 94,\n",
       " 19,\n",
       " 381,\n",
       " 264,\n",
       " 33,\n",
       " 16,\n",
       " 14,\n",
       " 3,\n",
       " 257,\n",
       " 42,\n",
       " 382,\n",
       " 29,\n",
       " 154,\n",
       " 383,\n",
       " 146,\n",
       " 384,\n",
       " 5,\n",
       " 378,\n",
       " 310,\n",
       " 385,\n",
       " 119,\n",
       " 120,\n",
       " 356,\n",
       " 72,\n",
       " 3,\n",
       " 386,\n",
       " 6,\n",
       " 387,\n",
       " 120,\n",
       " 388,\n",
       " 42,\n",
       " 3,\n",
       " 122,\n",
       " 5,\n",
       " 3,\n",
       " 379,\n",
       " 3,\n",
       " 389,\n",
       " 20,\n",
       " 5,\n",
       " 94,\n",
       " 390,\n",
       " 33,\n",
       " 18,\n",
       " 142,\n",
       " 391,\n",
       " 78,\n",
       " 392,\n",
       " 393,\n",
       " 84,\n",
       " 394,\n",
       " 3,\n",
       " 395,\n",
       " 396,\n",
       " 397,\n",
       " 47,\n",
       " 398,\n",
       " 399,\n",
       " 400,\n",
       " 401,\n",
       " 8,\n",
       " 402,\n",
       " 403,\n",
       " 404,\n",
       " 42,\n",
       " 405,\n",
       " 219,\n",
       " 5,\n",
       " 406,\n",
       " 407,\n",
       " 64,\n",
       " 408,\n",
       " 80,\n",
       " 409,\n",
       " 225,\n",
       " 84,\n",
       " 111,\n",
       " 112,\n",
       " 122,\n",
       " 1,\n",
       " 66,\n",
       " 15,\n",
       " 410,\n",
       " 42,\n",
       " 411,\n",
       " 396,\n",
       " 207,\n",
       " 276,\n",
       " 412,\n",
       " 413,\n",
       " 414,\n",
       " 415,\n",
       " 44,\n",
       " 416,\n",
       " ...]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## array of words passed in models to train -- tokenization\n",
    "\n",
    "## we pass no (give no to each word called embedding)\n",
    "## for each word assigning unique number\n",
    "## after tokenizing assigning no to eaxh unique word\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "tokenizer.fit_on_texts([new_data])\n",
    "\n",
    "\n",
    "\n",
    "## converting 2d output to 1d\n",
    "\n",
    "sequence_data = tokenizer.texts_to_sequences([data])[0]\n",
    "sequence_data\n",
    "\n",
    "\n",
    "#[we, visit, agra, we, liked, agra]\n",
    "#[1 33 44 1 23 44]\n",
    "\n",
    "\n",
    "#WARM STATE=true\n",
    "#TRANSFER LEARNING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-07T10:04:13.863792Z",
     "start_time": "2020-11-07T10:04:13.856812Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1473"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## checking unique words\n",
    "\n",
    "len(sequence_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-07T10:04:16.710125Z",
     "start_time": "2020-11-07T10:04:16.702149Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('on', 1), ('minister', 2), ('the', 3), ('we', 4), ('in', 5), ('three', 6), ('saying', 7), ('an', 8), ('after', 9), ('when', 10), ('two', 11), ('over', 12), ('pm', 13), ('this', 14), ('as', 15), ('with', 16), ('according', 17), ('it', 18), ('health', 19), ('ministry', 20), ('capital', 21), ('new', 22), ('within', 23), ('grant', 24), ('finance', 25), ('narendra', 26), ('modi', 27), ('today', 28), ('coronavirus', 29), ('latest', 30), ('updates', 31), ('prime', 32), ('said', 33), ('that', 34), ('based', 35), ('suggestions', 36), ('by', 37), ('states', 38), ('information', 39), ('about', 40), ('extension', 41), ('of', 42), ('lockdown', 43), ('will', 44), ('be', 45), ('given', 46), ('before', 47), ('18', 48), ('may', 49), ('fight', 50), ('corona', 51), ('and', 52), ('move', 53), ('forward', 54), ('he', 55), ('added', 56), ('must', 57), ('not', 58), ('restrict', 59), ('our', 60), ('lives', 61), ('only', 62), ('around', 63), ('his', 64), ('speech', 65), ('tuesday', 66), ('fourth', 67), ('phase', 68), ('india', 69), ('completely', 70), ('different', 71), ('from', 72), ('earlier', 73), ('phases', 74), ('has', 75), ('been', 76), ('example', 77), ('for', 78), ('progress', 79), ('last', 80), ('century', 81), ('country', 82), ('needs', 83), ('to', 84), ('become', 85), ('self', 86), ('reliant', 87), ('world', 88), ('pandemic', 89), ('crisis', 90), ('started', 91), ('then', 92), ('even', 93), ('a', 94), ('single', 95), ('ppe', 96), ('kit', 97), ('was', 98), ('manufactured', 99), ('few', 100), ('n95', 101), ('masks', 102), ('were', 103), ('available', 104), ('lakh', 105), ('kits', 106), ('2', 107), ('are', 108), ('daily', 109), ('speaking', 110), ('covid', 111), ('19', 112), ('situation', 113), ('battling', 114), ('four', 115), ('months', 116), ('now', 117), ('3', 118), ('people', 119), ('have', 120), ('succumbed', 121), ('infection', 122), ('says', 123), ('condoling', 124), ('deaths', 125), ('approved', 126), ('ex', 127), ('gratia', 128), ('rs', 129), ('each', 130), ('next', 131), ('kin', 132), ('16', 133), ('migrants', 134), ('who', 135), ('run', 136), ('goods', 137), ('train', 138), ('near', 139), ('aurangabad', 140), ('maharashtra', 141), ('is', 142), ('set', 143), ('address', 144), ('nation', 145), ('at', 146), ('8', 147), ('s', 148), ('fifth', 149), ('comes', 150), ('time', 151), ('70', 152), ('000', 153), ('cases', 154), ('west', 155), ('bengal', 156), ('chief', 157), ('mamata', 158), ('banerjee', 159), ('relief', 160), ('unlikely', 161), ('soon', 162), ('there', 163), ('need', 164), ('month', 165), ('plan', 166), ('deal', 167), ('also', 168), ('quoted', 169), ('facing', 170), ('problems', 171), ('since', 172), ('imposition', 173), ('25', 174), ('march', 175), ('poorly', 176), ('planned', 177), ('news18', 178), ('reported', 179), ('government', 180), ('allowed', 181), ('home', 182), ('delivery', 183), ('alcohol', 184), ('guidelines', 185), ('order', 186), ('issued', 187), ('can', 188), ('made', 189), ('into', 190), ('area', 191), ('license', 192), ('doing', 193), ('wear', 194), ('mask', 195), ('use', 196), ('sanitisers', 197), ('ani', 198), ('sources', 199), ('second', 200), ('centre', 201), ('repatriation', 202), ('exercise', 203), ('dubbed', 204), ('vande', 205), ('bharat', 206), ('mission', 207), ('launched', 208), ('22', 209), ('bring', 210), ('back', 211), ('indians', 212), ('31', 213), ('countries', 214), ('149', 215), ('flights', 216), ('including', 217), ('feeder', 218), ('deployed', 219), ('report', 220), ('harsh', 221), ('vardhan', 222), ('mortality', 223), ('rate', 224), ('due', 225), ('one', 226), ('lowest', 227), ('globally', 228), ('against', 229), ('percent', 230), ('several', 231), ('less', 232), ('than', 233), ('global', 234), ('fatality', 235), ('7', 236), ('5', 237), ('reports', 238), ('17', 239), ('prisoners', 240), ('—', 241), ('almost', 242), ('50', 243), ('parole', 244), ('maintain', 245), ('social', 246), ('distancing', 247), ('decongesting', 248), ('prisons', 249), ('81', 250), ('more', 251), ('individuals', 252), ('tested', 253), ('positive', 254), ('indore', 255), ('total', 256), ('number', 257), ('confirmed', 258), ('madhya', 259), ('pradesh', 260), ('district', 261), ('climbed', 262), ('016', 263), ('official', 264), ('toll', 265), ('reached', 266), ('92', 267), ('fatalities', 268), ('registered', 269), ('past', 270), ('24', 271), ('hours', 272), ('many', 273), ('6', 274), ('037', 275), ('indian', 276), ('nationals', 277), ('evacuated', 278), ('inbound', 279), ('operated', 280), ('air', 281), ('express', 282), ('under', 283), ('civil', 284), ('aviation', 285), ('operation', 286), ('began', 287), ('five', 288), ('days', 289), ('ago', 290), ('nearly', 291), ('hearing', 292), ('extremely', 293), ('urgent', 294), ('matters', 295), ('supreme', 296), ('court', 297), ('likely', 298), ('hear', 299), ('through', 300), ('video', 301), ('conferencing', 302), ('large', 303), ('scale', 304), ('bar', 305), ('bench', 306), ('observation', 307), ('effect', 308), ('judge', 309), ('while', 310), ('bail', 311), ('application', 312), ('justices', 313), ('l', 314), ('nageshwara', 315), ('rao', 316), ('abdul', 317), ('nazeer', 318), ('sanjiv', 319), ('khanna', 320), ('hinted', 321), ('same', 322), ('observed', 323), ('matter', 324), ('would', 325), ('taken', 326), ('up', 327), ('urgently', 328), ('hearings', 329), ('begin', 330), ('larger', 331), ('current', 332), ('doubling', 333), ('delhi', 334), ('stands', 335), ('11', 336), ('satyendra', 337), ('jain', 338), ('had', 339), ('once', 340), ('or', 341), ('4', 342), ('if', 343), ('reaches', 344), ('20', 345), ('comfortable', 346), ('recovery', 347), ('national', 348), ('33', 349), ('512', 350), ('patients', 351), ('cured', 352), ('383', 353), ('discharged', 354), ('taking', 355), ('recovered', 356), ('2512', 357), ('further', 358), ('406', 359), ('testing', 360), ('novel', 361), ('639', 362), ('union', 363), ('territory', 364), ('86', 365), ('13', 366), ('yesterday', 367), ('tweeted', 368), ('office', 369), ('discuss', 370), ('possible', 371), ('imposed', 372), ('view', 373), ('red', 374), ('zone', 375), ('districts', 376), ('twenty', 377), ('odisha', 378), ('state', 379), ('437', 380), ('department', 381), ('active', 382), ('stood', 383), ('349', 384), ('85', 385), ('disease', 386), ('persons', 387), ('died', 388), ('railway', 389), ('tweet', 390), ('compulsory', 391), ('all', 392), ('passengers', 393), ('download', 394), ('aarogya', 395), ('setu', 396), ('app', 397), ('commencing', 398), ('their', 399), ('journey', 400), ('meanwhile', 401), ('assistant', 402), ('sub', 403), ('inspector', 404), ('cisf', 405), ('kolkata', 406), ('lost', 407), ('life', 408), ('night', 409), ('part', 410), ('samudra', 411), ('navy', 412), ('ship', 413), ('ins', 414), ('jalashwa', 415), ('sail', 416), ('again', 417), ('maldives', 418), ('male', 419), ('friday', 420), ('bringing', 421), ('residents', 422), ('kerala', 423), ('lakshadweep', 424), ('brought', 425), ('698', 426), ('kochi', 427), ('sunday', 428), ('its', 429), ('ferry', 430), ('scheduled', 431), ('tuticorin', 432), ('but', 433), ('destination', 434), ('changed', 435), ('necessary', 436), ('approvals', 437), ('tamil', 438), ('nadu', 439), ('received', 440), ('47', 441), ('rajasthan', 442), ('305', 443), ('took', 444), ('across', 445), ('151', 446), ('756', 447), ('604', 448), ('293', 449), ('figure', 450), ('includes', 451), ('46', 452), ('008', 453), ('data', 454), ('released', 455), ('family', 456), ('welfare', 457), ('454', 458), ('infectious', 459), ('passenger', 460), ('services', 461), ('railways', 462), ('monday', 463), ('reservations', 464), ('54', 465), ('minutes', 466), ('booking', 467), ('tickets', 468), ('ac', 469), ('classes', 470), ('sold', 471), ('out', 472), ('mumbai', 473), ('central', 474), ('special', 475), ('till', 476), ('times', 477), ('195', 478), ('08', 479), ('crore', 480), ('14', 481), ('equated', 482), ('monthly', 483), ('installment', 484), ('post', 485), ('devolution', 486), ('revenue', 487), ('deficit', 488), ('provide', 489), ('them', 490), ('additional', 491), ('resources', 492), ('during', 493), ('statement', 494), ('recommended', 495), ('15th', 496), ('commission', 497), ('equal', 498), ('first', 499), ('advance', 500), ('payments', 501), ('1', 502), ('276', 503), ('followed', 504), ('952', 505), ('himachal', 506), ('638', 507), ('punjab', 508), ('assam', 509), ('631', 510), ('andhra', 511), ('491', 512), ('uttarakhand', 513), ('423', 514), ('got', 515), ('417', 516), ('record', 517), ('jump', 518), ('213', 519), ('virtual', 520), ('interaction', 521), ('ministers', 522), ('biggest', 523), ('challenge', 524), ('ensure', 525), ('does', 526), ('spread', 527), ('rural', 528), ('devise', 529), ('balanced', 530), ('strategy', 531), ('step', 532), ('economic', 533), ('activities', 534), ('gradual', 535), ('manner', 536), ('separately', 537), ('some', 538), ('relatively', 539), ('outbreaks', 540), ('noticed', 541), ('particular', 542), ('locations', 543), ('important', 544), ('focus', 545), ('containment', 546), ('efforts', 547), ('reach', 548), ('community', 549), ('transmission', 550), ('stage', 551)])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## word wise no assigned \n",
    "tokenizer.word_index.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-07T10:04:18.874389Z",
     "start_time": "2020-11-07T10:04:18.868406Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "552\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "print(vocab_size)\n",
    "\n",
    "\n",
    "## because index starts from 0 -- 551"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-07T10:04:21.419142Z",
     "start_time": "2020-11-07T10:04:21.385234Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'on': 1,\n",
       " 'minister': 2,\n",
       " 'the': 3,\n",
       " 'we': 4,\n",
       " 'in': 5,\n",
       " 'three': 6,\n",
       " 'saying': 7,\n",
       " 'an': 8,\n",
       " 'after': 9,\n",
       " 'when': 10,\n",
       " 'two': 11,\n",
       " 'over': 12,\n",
       " 'pm': 13,\n",
       " 'this': 14,\n",
       " 'as': 15,\n",
       " 'with': 16,\n",
       " 'according': 17,\n",
       " 'it': 18,\n",
       " 'health': 19,\n",
       " 'ministry': 20,\n",
       " 'capital': 21,\n",
       " 'new': 22,\n",
       " 'within': 23,\n",
       " 'grant': 24,\n",
       " 'finance': 25,\n",
       " 'narendra': 26,\n",
       " 'modi': 27,\n",
       " 'today': 28,\n",
       " 'coronavirus': 29,\n",
       " 'latest': 30,\n",
       " 'updates': 31,\n",
       " 'prime': 32,\n",
       " 'said': 33,\n",
       " 'that': 34,\n",
       " 'based': 35,\n",
       " 'suggestions': 36,\n",
       " 'by': 37,\n",
       " 'states': 38,\n",
       " 'information': 39,\n",
       " 'about': 40,\n",
       " 'extension': 41,\n",
       " 'of': 42,\n",
       " 'lockdown': 43,\n",
       " 'will': 44,\n",
       " 'be': 45,\n",
       " 'given': 46,\n",
       " 'before': 47,\n",
       " '18': 48,\n",
       " 'may': 49,\n",
       " 'fight': 50,\n",
       " 'corona': 51,\n",
       " 'and': 52,\n",
       " 'move': 53,\n",
       " 'forward': 54,\n",
       " 'he': 55,\n",
       " 'added': 56,\n",
       " 'must': 57,\n",
       " 'not': 58,\n",
       " 'restrict': 59,\n",
       " 'our': 60,\n",
       " 'lives': 61,\n",
       " 'only': 62,\n",
       " 'around': 63,\n",
       " 'his': 64,\n",
       " 'speech': 65,\n",
       " 'tuesday': 66,\n",
       " 'fourth': 67,\n",
       " 'phase': 68,\n",
       " 'india': 69,\n",
       " 'completely': 70,\n",
       " 'different': 71,\n",
       " 'from': 72,\n",
       " 'earlier': 73,\n",
       " 'phases': 74,\n",
       " 'has': 75,\n",
       " 'been': 76,\n",
       " 'example': 77,\n",
       " 'for': 78,\n",
       " 'progress': 79,\n",
       " 'last': 80,\n",
       " 'century': 81,\n",
       " 'country': 82,\n",
       " 'needs': 83,\n",
       " 'to': 84,\n",
       " 'become': 85,\n",
       " 'self': 86,\n",
       " 'reliant': 87,\n",
       " 'world': 88,\n",
       " 'pandemic': 89,\n",
       " 'crisis': 90,\n",
       " 'started': 91,\n",
       " 'then': 92,\n",
       " 'even': 93,\n",
       " 'a': 94,\n",
       " 'single': 95,\n",
       " 'ppe': 96,\n",
       " 'kit': 97,\n",
       " 'was': 98,\n",
       " 'manufactured': 99,\n",
       " 'few': 100,\n",
       " 'n95': 101,\n",
       " 'masks': 102,\n",
       " 'were': 103,\n",
       " 'available': 104,\n",
       " 'lakh': 105,\n",
       " 'kits': 106,\n",
       " '2': 107,\n",
       " 'are': 108,\n",
       " 'daily': 109,\n",
       " 'speaking': 110,\n",
       " 'covid': 111,\n",
       " '19': 112,\n",
       " 'situation': 113,\n",
       " 'battling': 114,\n",
       " 'four': 115,\n",
       " 'months': 116,\n",
       " 'now': 117,\n",
       " '3': 118,\n",
       " 'people': 119,\n",
       " 'have': 120,\n",
       " 'succumbed': 121,\n",
       " 'infection': 122,\n",
       " 'says': 123,\n",
       " 'condoling': 124,\n",
       " 'deaths': 125,\n",
       " 'approved': 126,\n",
       " 'ex': 127,\n",
       " 'gratia': 128,\n",
       " 'rs': 129,\n",
       " 'each': 130,\n",
       " 'next': 131,\n",
       " 'kin': 132,\n",
       " '16': 133,\n",
       " 'migrants': 134,\n",
       " 'who': 135,\n",
       " 'run': 136,\n",
       " 'goods': 137,\n",
       " 'train': 138,\n",
       " 'near': 139,\n",
       " 'aurangabad': 140,\n",
       " 'maharashtra': 141,\n",
       " 'is': 142,\n",
       " 'set': 143,\n",
       " 'address': 144,\n",
       " 'nation': 145,\n",
       " 'at': 146,\n",
       " '8': 147,\n",
       " 's': 148,\n",
       " 'fifth': 149,\n",
       " 'comes': 150,\n",
       " 'time': 151,\n",
       " '70': 152,\n",
       " '000': 153,\n",
       " 'cases': 154,\n",
       " 'west': 155,\n",
       " 'bengal': 156,\n",
       " 'chief': 157,\n",
       " 'mamata': 158,\n",
       " 'banerjee': 159,\n",
       " 'relief': 160,\n",
       " 'unlikely': 161,\n",
       " 'soon': 162,\n",
       " 'there': 163,\n",
       " 'need': 164,\n",
       " 'month': 165,\n",
       " 'plan': 166,\n",
       " 'deal': 167,\n",
       " 'also': 168,\n",
       " 'quoted': 169,\n",
       " 'facing': 170,\n",
       " 'problems': 171,\n",
       " 'since': 172,\n",
       " 'imposition': 173,\n",
       " '25': 174,\n",
       " 'march': 175,\n",
       " 'poorly': 176,\n",
       " 'planned': 177,\n",
       " 'news18': 178,\n",
       " 'reported': 179,\n",
       " 'government': 180,\n",
       " 'allowed': 181,\n",
       " 'home': 182,\n",
       " 'delivery': 183,\n",
       " 'alcohol': 184,\n",
       " 'guidelines': 185,\n",
       " 'order': 186,\n",
       " 'issued': 187,\n",
       " 'can': 188,\n",
       " 'made': 189,\n",
       " 'into': 190,\n",
       " 'area': 191,\n",
       " 'license': 192,\n",
       " 'doing': 193,\n",
       " 'wear': 194,\n",
       " 'mask': 195,\n",
       " 'use': 196,\n",
       " 'sanitisers': 197,\n",
       " 'ani': 198,\n",
       " 'sources': 199,\n",
       " 'second': 200,\n",
       " 'centre': 201,\n",
       " 'repatriation': 202,\n",
       " 'exercise': 203,\n",
       " 'dubbed': 204,\n",
       " 'vande': 205,\n",
       " 'bharat': 206,\n",
       " 'mission': 207,\n",
       " 'launched': 208,\n",
       " '22': 209,\n",
       " 'bring': 210,\n",
       " 'back': 211,\n",
       " 'indians': 212,\n",
       " '31': 213,\n",
       " 'countries': 214,\n",
       " '149': 215,\n",
       " 'flights': 216,\n",
       " 'including': 217,\n",
       " 'feeder': 218,\n",
       " 'deployed': 219,\n",
       " 'report': 220,\n",
       " 'harsh': 221,\n",
       " 'vardhan': 222,\n",
       " 'mortality': 223,\n",
       " 'rate': 224,\n",
       " 'due': 225,\n",
       " 'one': 226,\n",
       " 'lowest': 227,\n",
       " 'globally': 228,\n",
       " 'against': 229,\n",
       " 'percent': 230,\n",
       " 'several': 231,\n",
       " 'less': 232,\n",
       " 'than': 233,\n",
       " 'global': 234,\n",
       " 'fatality': 235,\n",
       " '7': 236,\n",
       " '5': 237,\n",
       " 'reports': 238,\n",
       " '17': 239,\n",
       " 'prisoners': 240,\n",
       " '—': 241,\n",
       " 'almost': 242,\n",
       " '50': 243,\n",
       " 'parole': 244,\n",
       " 'maintain': 245,\n",
       " 'social': 246,\n",
       " 'distancing': 247,\n",
       " 'decongesting': 248,\n",
       " 'prisons': 249,\n",
       " '81': 250,\n",
       " 'more': 251,\n",
       " 'individuals': 252,\n",
       " 'tested': 253,\n",
       " 'positive': 254,\n",
       " 'indore': 255,\n",
       " 'total': 256,\n",
       " 'number': 257,\n",
       " 'confirmed': 258,\n",
       " 'madhya': 259,\n",
       " 'pradesh': 260,\n",
       " 'district': 261,\n",
       " 'climbed': 262,\n",
       " '016': 263,\n",
       " 'official': 264,\n",
       " 'toll': 265,\n",
       " 'reached': 266,\n",
       " '92': 267,\n",
       " 'fatalities': 268,\n",
       " 'registered': 269,\n",
       " 'past': 270,\n",
       " '24': 271,\n",
       " 'hours': 272,\n",
       " 'many': 273,\n",
       " '6': 274,\n",
       " '037': 275,\n",
       " 'indian': 276,\n",
       " 'nationals': 277,\n",
       " 'evacuated': 278,\n",
       " 'inbound': 279,\n",
       " 'operated': 280,\n",
       " 'air': 281,\n",
       " 'express': 282,\n",
       " 'under': 283,\n",
       " 'civil': 284,\n",
       " 'aviation': 285,\n",
       " 'operation': 286,\n",
       " 'began': 287,\n",
       " 'five': 288,\n",
       " 'days': 289,\n",
       " 'ago': 290,\n",
       " 'nearly': 291,\n",
       " 'hearing': 292,\n",
       " 'extremely': 293,\n",
       " 'urgent': 294,\n",
       " 'matters': 295,\n",
       " 'supreme': 296,\n",
       " 'court': 297,\n",
       " 'likely': 298,\n",
       " 'hear': 299,\n",
       " 'through': 300,\n",
       " 'video': 301,\n",
       " 'conferencing': 302,\n",
       " 'large': 303,\n",
       " 'scale': 304,\n",
       " 'bar': 305,\n",
       " 'bench': 306,\n",
       " 'observation': 307,\n",
       " 'effect': 308,\n",
       " 'judge': 309,\n",
       " 'while': 310,\n",
       " 'bail': 311,\n",
       " 'application': 312,\n",
       " 'justices': 313,\n",
       " 'l': 314,\n",
       " 'nageshwara': 315,\n",
       " 'rao': 316,\n",
       " 'abdul': 317,\n",
       " 'nazeer': 318,\n",
       " 'sanjiv': 319,\n",
       " 'khanna': 320,\n",
       " 'hinted': 321,\n",
       " 'same': 322,\n",
       " 'observed': 323,\n",
       " 'matter': 324,\n",
       " 'would': 325,\n",
       " 'taken': 326,\n",
       " 'up': 327,\n",
       " 'urgently': 328,\n",
       " 'hearings': 329,\n",
       " 'begin': 330,\n",
       " 'larger': 331,\n",
       " 'current': 332,\n",
       " 'doubling': 333,\n",
       " 'delhi': 334,\n",
       " 'stands': 335,\n",
       " '11': 336,\n",
       " 'satyendra': 337,\n",
       " 'jain': 338,\n",
       " 'had': 339,\n",
       " 'once': 340,\n",
       " 'or': 341,\n",
       " '4': 342,\n",
       " 'if': 343,\n",
       " 'reaches': 344,\n",
       " '20': 345,\n",
       " 'comfortable': 346,\n",
       " 'recovery': 347,\n",
       " 'national': 348,\n",
       " '33': 349,\n",
       " '512': 350,\n",
       " 'patients': 351,\n",
       " 'cured': 352,\n",
       " '383': 353,\n",
       " 'discharged': 354,\n",
       " 'taking': 355,\n",
       " 'recovered': 356,\n",
       " '2512': 357,\n",
       " 'further': 358,\n",
       " '406': 359,\n",
       " 'testing': 360,\n",
       " 'novel': 361,\n",
       " '639': 362,\n",
       " 'union': 363,\n",
       " 'territory': 364,\n",
       " '86': 365,\n",
       " '13': 366,\n",
       " 'yesterday': 367,\n",
       " 'tweeted': 368,\n",
       " 'office': 369,\n",
       " 'discuss': 370,\n",
       " 'possible': 371,\n",
       " 'imposed': 372,\n",
       " 'view': 373,\n",
       " 'red': 374,\n",
       " 'zone': 375,\n",
       " 'districts': 376,\n",
       " 'twenty': 377,\n",
       " 'odisha': 378,\n",
       " 'state': 379,\n",
       " '437': 380,\n",
       " 'department': 381,\n",
       " 'active': 382,\n",
       " 'stood': 383,\n",
       " '349': 384,\n",
       " '85': 385,\n",
       " 'disease': 386,\n",
       " 'persons': 387,\n",
       " 'died': 388,\n",
       " 'railway': 389,\n",
       " 'tweet': 390,\n",
       " 'compulsory': 391,\n",
       " 'all': 392,\n",
       " 'passengers': 393,\n",
       " 'download': 394,\n",
       " 'aarogya': 395,\n",
       " 'setu': 396,\n",
       " 'app': 397,\n",
       " 'commencing': 398,\n",
       " 'their': 399,\n",
       " 'journey': 400,\n",
       " 'meanwhile': 401,\n",
       " 'assistant': 402,\n",
       " 'sub': 403,\n",
       " 'inspector': 404,\n",
       " 'cisf': 405,\n",
       " 'kolkata': 406,\n",
       " 'lost': 407,\n",
       " 'life': 408,\n",
       " 'night': 409,\n",
       " 'part': 410,\n",
       " 'samudra': 411,\n",
       " 'navy': 412,\n",
       " 'ship': 413,\n",
       " 'ins': 414,\n",
       " 'jalashwa': 415,\n",
       " 'sail': 416,\n",
       " 'again': 417,\n",
       " 'maldives': 418,\n",
       " 'male': 419,\n",
       " 'friday': 420,\n",
       " 'bringing': 421,\n",
       " 'residents': 422,\n",
       " 'kerala': 423,\n",
       " 'lakshadweep': 424,\n",
       " 'brought': 425,\n",
       " '698': 426,\n",
       " 'kochi': 427,\n",
       " 'sunday': 428,\n",
       " 'its': 429,\n",
       " 'ferry': 430,\n",
       " 'scheduled': 431,\n",
       " 'tuticorin': 432,\n",
       " 'but': 433,\n",
       " 'destination': 434,\n",
       " 'changed': 435,\n",
       " 'necessary': 436,\n",
       " 'approvals': 437,\n",
       " 'tamil': 438,\n",
       " 'nadu': 439,\n",
       " 'received': 440,\n",
       " '47': 441,\n",
       " 'rajasthan': 442,\n",
       " '305': 443,\n",
       " 'took': 444,\n",
       " 'across': 445,\n",
       " '151': 446,\n",
       " '756': 447,\n",
       " '604': 448,\n",
       " '293': 449,\n",
       " 'figure': 450,\n",
       " 'includes': 451,\n",
       " '46': 452,\n",
       " '008': 453,\n",
       " 'data': 454,\n",
       " 'released': 455,\n",
       " 'family': 456,\n",
       " 'welfare': 457,\n",
       " '454': 458,\n",
       " 'infectious': 459,\n",
       " 'passenger': 460,\n",
       " 'services': 461,\n",
       " 'railways': 462,\n",
       " 'monday': 463,\n",
       " 'reservations': 464,\n",
       " '54': 465,\n",
       " 'minutes': 466,\n",
       " 'booking': 467,\n",
       " 'tickets': 468,\n",
       " 'ac': 469,\n",
       " 'classes': 470,\n",
       " 'sold': 471,\n",
       " 'out': 472,\n",
       " 'mumbai': 473,\n",
       " 'central': 474,\n",
       " 'special': 475,\n",
       " 'till': 476,\n",
       " 'times': 477,\n",
       " '195': 478,\n",
       " '08': 479,\n",
       " 'crore': 480,\n",
       " '14': 481,\n",
       " 'equated': 482,\n",
       " 'monthly': 483,\n",
       " 'installment': 484,\n",
       " 'post': 485,\n",
       " 'devolution': 486,\n",
       " 'revenue': 487,\n",
       " 'deficit': 488,\n",
       " 'provide': 489,\n",
       " 'them': 490,\n",
       " 'additional': 491,\n",
       " 'resources': 492,\n",
       " 'during': 493,\n",
       " 'statement': 494,\n",
       " 'recommended': 495,\n",
       " '15th': 496,\n",
       " 'commission': 497,\n",
       " 'equal': 498,\n",
       " 'first': 499,\n",
       " 'advance': 500,\n",
       " 'payments': 501,\n",
       " '1': 502,\n",
       " '276': 503,\n",
       " 'followed': 504,\n",
       " '952': 505,\n",
       " 'himachal': 506,\n",
       " '638': 507,\n",
       " 'punjab': 508,\n",
       " 'assam': 509,\n",
       " '631': 510,\n",
       " 'andhra': 511,\n",
       " '491': 512,\n",
       " 'uttarakhand': 513,\n",
       " '423': 514,\n",
       " 'got': 515,\n",
       " '417': 516,\n",
       " 'record': 517,\n",
       " 'jump': 518,\n",
       " '213': 519,\n",
       " 'virtual': 520,\n",
       " 'interaction': 521,\n",
       " 'ministers': 522,\n",
       " 'biggest': 523,\n",
       " 'challenge': 524,\n",
       " 'ensure': 525,\n",
       " 'does': 526,\n",
       " 'spread': 527,\n",
       " 'rural': 528,\n",
       " 'devise': 529,\n",
       " 'balanced': 530,\n",
       " 'strategy': 531,\n",
       " 'step': 532,\n",
       " 'economic': 533,\n",
       " 'activities': 534,\n",
       " 'gradual': 535,\n",
       " 'manner': 536,\n",
       " 'separately': 537,\n",
       " 'some': 538,\n",
       " 'relatively': 539,\n",
       " 'outbreaks': 540,\n",
       " 'noticed': 541,\n",
       " 'particular': 542,\n",
       " 'locations': 543,\n",
       " 'important': 544,\n",
       " 'focus': 545,\n",
       " 'containment': 546,\n",
       " 'efforts': 547,\n",
       " 'reach': 548,\n",
       " 'community': 549,\n",
       " 'transmission': 550,\n",
       " 'stage': 551}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-07T10:04:23.604936Z",
     "start_time": "2020-11-07T10:04:23.591969Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Length of sequences are:  1472\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[26, 27],\n",
       "       [27, 28],\n",
       "       [28,  1],\n",
       "       [ 1, 29],\n",
       "       [29, 30],\n",
       "       [30, 31],\n",
       "       [31, 32],\n",
       "       [32,  2],\n",
       "       [ 2, 26],\n",
       "       [26, 27]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## pairing words in 2d array\n",
    "## preparing train and test data\n",
    "sequences = []\n",
    "\n",
    "for i in range(1, len(sequence_data)):\n",
    "    words = sequence_data[i-1:i+1]\n",
    "    sequences.append(words)\n",
    "    \n",
    "print(\"The Length of sequences are: \", len(sequences))\n",
    "\n",
    "sequences = np.array(sequences)\n",
    "\n",
    "sequences[:10]\n",
    "\n",
    "\n",
    "#[17, 53, 293, 2, 18, 729, 135, 730, 294, 8]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-07T10:04:25.473959Z",
     "start_time": "2020-11-07T10:04:25.465981Z"
    }
   },
   "outputs": [],
   "source": [
    "## 0 index of 2 d array as x and 1st index as y\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for i in sequences:\n",
    "    x.append(i[0])\n",
    "    y.append(i[1])\n",
    "    \n",
    "x = np.array(x)\n",
    "y = np.array(y)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-07T10:04:28.644454Z",
     "start_time": "2020-11-07T10:04:28.638475Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1472,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-07T10:04:30.315406Z",
     "start_time": "2020-11-07T10:04:30.308426Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1472,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-07T10:04:40.255282Z",
     "start_time": "2020-11-07T10:04:40.247303Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Data is:  [26 27 28  1 29]\n",
      "The responses are:  [27 28  1 29 30]\n"
     ]
    }
   ],
   "source": [
    "print(\"The Data is: \", x[:5])\n",
    "\n",
    "print(\"The responses are: \", y[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-07T10:04:42.815437Z",
     "start_time": "2020-11-07T10:04:42.805461Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## embedding layer automaticatley covert x into vectors of 0,1,0,1(converting input space into vector space) and generate vector of\n",
    "## output 0,1 so need to convert y into 0,1 vectors\n",
    "## embedding layers k through pass krega those vectors\n",
    "\n",
    "## need to convert output y on our own so that easily compared with output vectors\n",
    "\n",
    "## in keras we have to_categorically like one hot encoding (creating one hot encoding space)\n",
    "#In rows we have vocab and columns we have words, matching rows = columns is 1 and rest 0\n",
    "\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "\n",
    "y[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-07T10:04:46.106051Z",
     "start_time": "2020-11-07T10:04:46.100067Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1472, 552)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding or embedding =-------for vector representation\n",
    "## each no into a vector - embedding will do\n",
    "\n",
    "## one hot encoding problem - it creates a sparse matrix\n",
    "## eg 8 no is represnted as 10 dim vectors will contain 10 rows out of which 8th index has value 1 rest is 0\n",
    "##(no needs to be represntd as vector)\n",
    "\n",
    "\n",
    "## instead of 0 1 we have more relevant i.e. emebedding\n",
    "#NLP - WORD EMBEDDING FOUNDATION\n",
    "##embedding automotictely do it for u\n",
    "## packages for embedding on ur own - word2vec and Glove\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-07T10:04:48.895405Z",
     "start_time": "2020-11-07T10:04:48.891416Z"
    }
   },
   "outputs": [],
   "source": [
    "## embedding is a special layer - It is going to play role of input layer in a different way\n",
    "## part of hidden layer - direct relation with input layer \n",
    "\n",
    "## processiong in embedding layer thats 'why its's a hidden layer\n",
    "## work for embedding layer to generate sequence for the lstm layer\n",
    "\n",
    "## you need to set return_sequences = true (to preseve that sequence coming from embedding layer)in lstm because they understand sequence, \n",
    "## when one layer of lstm going to another layer of lstm then we need to preserve the sequence (return_sequences)\n",
    "\n",
    "## vocab_size = output layer - for each word we will predict a prob(given word what is the next word), max prob will be the output\n",
    "\n",
    "## first layer embedding and lst layer is mandatory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-07T10:04:54.538450Z",
     "start_time": "2020-11-07T10:04:51.535256Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(vocab_size,10,input_length = 1))\n",
    "\n",
    "#vocab_size = input_dim\n",
    "# 10 - output_dim\n",
    "#input length of user should be 1 to predict next word\n",
    "\n",
    "model.add(LSTM(1000,  return_sequences = True))\n",
    "model.add(LSTM(1000))\n",
    "\n",
    "model.add(Dense(1000, activation = 'relu'))\n",
    "\n",
    "model.add(Dense(vocab_size, activation = 'softmax'))\n",
    "          \n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-07T10:04:57.884547Z",
     "start_time": "2020-11-07T10:04:57.875571Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 1, 10)             5520      \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 1, 1000)           4044000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 1000)              8004000   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 552)               552552    \n",
      "=================================================================\n",
      "Total params: 13,607,072\n",
      "Trainable params: 13,607,072\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Plotting model - Import Libraries\n",
    "#### You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-07T10:05:05.193064Z",
     "start_time": "2020-11-07T10:05:04.992603Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "keras.utils.plot_model(model, to_file = 'model.png', show_layer_names = True,show_shapes=True,    expand_nested=True\n",
    ",rankdir='TB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whenever we train model, model call these funct from back -- call backs\n",
    "\n",
    "## writing the functions within in a array and late calling them after every epoch\n",
    "## when first epoch get completed for next epoch these all funct get called\n",
    "\n",
    "## checkpoint model - if loss gets decrese that state wil be included inn h5 file\n",
    "## getting updated after every epoch - h5 file\n",
    "\n",
    "## keras - library\n",
    "## tensorflow - framework\n",
    "\n",
    "## library - wrapper that gives some funct to call, call the underline funct from framework\n",
    "## framework - that actually interacts with machine(coding underline mathematial matrix)\n",
    "\n",
    "## all those embedded related vectors were tensors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Callbacks : Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-07T10:32:47.009671Z",
     "start_time": "2020-11-07T10:32:47.003686Z"
    }
   },
   "outputs": [],
   "source": [
    "## Output files for future predictions\n",
    "\n",
    "## To save the current state of model in a new file(saving the state of model) we use checkpoint\n",
    "## this model you can share this with ur team \n",
    "\n",
    "## saving the state of model , we can't train this again and again so saving it\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint = ModelCheckpoint('nextword1.h5', monitor = 'loss', verbose = True, save_best_only = True, mode = 'auto')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calbacks : ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-07T10:32:49.225622Z",
     "start_time": "2020-11-07T10:32:49.218641Z"
    }
   },
   "outputs": [],
   "source": [
    "##reduce learning rate on plateau - loss funct is decreasing, after some time it becomes straight called plateau\n",
    "## loss is not decreasing anymore - should model terminate? here we ask neural network to be intelligent we ask to reduce learning rate,\n",
    "##i.e. start taking smaller jumps to reach global minima\n",
    "## it takes certain parameters - monitor loss funct(loss funct normally decrease and see if it is coming to plateau)\n",
    "##lr - 0.03 initially setting patience as 0.22 then new lr becomes 0.03*2\n",
    "\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "reduce = ReduceLROnPlateau(monitor = 'loss', factor = 0.2, patience = 3, min_lr = 0.0001, verbose = True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks : TensorBoard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-07T10:37:15.293470Z",
     "start_time": "2020-11-07T10:37:15.283497Z"
    }
   },
   "outputs": [],
   "source": [
    "#Tensorboard - to print your graphs of accuracy and other variables\n",
    "##tensorboard visualization --- in tensoroard\n",
    "\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "logdir='logsnextword'\n",
    "tensorboard_vis = TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Also saving the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-07T09:55:49.269332Z",
     "start_time": "2020-11-07T09:55:49.261353Z"
    }
   },
   "outputs": [],
   "source": [
    "# saving the tokenizer for predict function.\n",
    "## saving tokenization so that it assign same no for the future purpose of the same word\n",
    "## so that numbering of words is not lost embedding layer with give different vector outputs\n",
    "\n",
    "import pickle\n",
    "\n",
    "## dumming the tokenizer into the model\n",
    "pickle.dump(tokenizer, open('tokenizer.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-07T10:38:00.804776Z",
     "start_time": "2020-11-07T10:38:00.799790Z"
    }
   },
   "outputs": [],
   "source": [
    "### callbacks list\n",
    "\n",
    "callbacks_list = [checkpoint, reduce, tensorboard_vis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For generic python process will save in - .pkl file \n",
    "#### For model processing will be saved in - pml or .h5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-07T10:38:06.175693Z",
     "start_time": "2020-11-07T10:38:06.145775Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-07T10:48:05.679326Z",
     "start_time": "2020-11-07T10:38:07.877961Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      " 1/23 [>.............................] - ETA: 0s - loss: 1.7421WARNING:tensorflow:From C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "23/23 [==============================] - ETA: 0s - loss: 2.0790\n",
      "Epoch 00001: loss improved from inf to 2.07903, saving model to nextword1.h5\n",
      "23/23 [==============================] - 4s 185ms/step - loss: 2.0790\n",
      "Epoch 2/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.8020\n",
      "Epoch 00002: loss improved from 2.07903 to 1.80204, saving model to nextword1.h5\n",
      "23/23 [==============================] - 4s 179ms/step - loss: 1.8020\n",
      "Epoch 3/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.7314\n",
      "Epoch 00003: loss improved from 1.80204 to 1.73139, saving model to nextword1.h5\n",
      "23/23 [==============================] - 4s 176ms/step - loss: 1.7314\n",
      "Epoch 4/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.7247\n",
      "Epoch 00004: loss improved from 1.73139 to 1.72467, saving model to nextword1.h5\n",
      "23/23 [==============================] - 4s 175ms/step - loss: 1.7247\n",
      "Epoch 5/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.7076\n",
      "Epoch 00005: loss improved from 1.72467 to 1.70759, saving model to nextword1.h5\n",
      "23/23 [==============================] - 4s 192ms/step - loss: 1.7076\n",
      "Epoch 6/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.7055\n",
      "Epoch 00006: loss improved from 1.70759 to 1.70551, saving model to nextword1.h5\n",
      "23/23 [==============================] - 4s 167ms/step - loss: 1.7055\n",
      "Epoch 7/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.6975\n",
      "Epoch 00007: loss improved from 1.70551 to 1.69751, saving model to nextword1.h5\n",
      "23/23 [==============================] - 4s 175ms/step - loss: 1.6975\n",
      "Epoch 8/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.6828\n",
      "Epoch 00008: loss improved from 1.69751 to 1.68278, saving model to nextword1.h5\n",
      "23/23 [==============================] - 4s 187ms/step - loss: 1.6828\n",
      "Epoch 9/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.6798\n",
      "Epoch 00009: loss improved from 1.68278 to 1.67982, saving model to nextword1.h5\n",
      "23/23 [==============================] - 4s 190ms/step - loss: 1.6798\n",
      "Epoch 10/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.6669\n",
      "Epoch 00010: loss improved from 1.67982 to 1.66689, saving model to nextword1.h5\n",
      "23/23 [==============================] - 5s 206ms/step - loss: 1.6669\n",
      "Epoch 11/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.6676\n",
      "Epoch 00011: loss did not improve from 1.66689\n",
      "23/23 [==============================] - 4s 173ms/step - loss: 1.6676\n",
      "Epoch 12/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.6742\n",
      "Epoch 00012: loss did not improve from 1.66689\n",
      "23/23 [==============================] - 4s 166ms/step - loss: 1.6742\n",
      "Epoch 13/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.6914- ETA: 0s - loss: \n",
      "Epoch 00013: loss did not improve from 1.66689\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "23/23 [==============================] - 4s 163ms/step - loss: 1.6914\n",
      "Epoch 14/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.5443\n",
      "Epoch 00014: loss improved from 1.66689 to 1.54429, saving model to nextword1.h5\n",
      "23/23 [==============================] - 4s 158ms/step - loss: 1.5443\n",
      "Epoch 15/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.5006- ETA: 0s - loss: 1\n",
      "Epoch 00015: loss improved from 1.54429 to 1.50056, saving model to nextword1.h5\n",
      "23/23 [==============================] - 4s 169ms/step - loss: 1.5006\n",
      "Epoch 16/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4842\n",
      "Epoch 00016: loss improved from 1.50056 to 1.48420, saving model to nextword1.h5\n",
      "23/23 [==============================] - 4s 159ms/step - loss: 1.4842\n",
      "Epoch 17/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4768\n",
      "Epoch 00017: loss improved from 1.48420 to 1.47682, saving model to nextword1.h5\n",
      "23/23 [==============================] - 5s 197ms/step - loss: 1.4768\n",
      "Epoch 18/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4716\n",
      "Epoch 00018: loss improved from 1.47682 to 1.47160, saving model to nextword1.h5\n",
      "23/23 [==============================] - 5s 205ms/step - loss: 1.4716\n",
      "Epoch 19/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4675\n",
      "Epoch 00019: loss improved from 1.47160 to 1.46749, saving model to nextword1.h5\n",
      "23/23 [==============================] - 5s 203ms/step - loss: 1.4675\n",
      "Epoch 20/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4661\n",
      "Epoch 00020: loss improved from 1.46749 to 1.46615, saving model to nextword1.h5\n",
      "23/23 [==============================] - 4s 193ms/step - loss: 1.4661\n",
      "Epoch 21/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4641\n",
      "Epoch 00021: loss improved from 1.46615 to 1.46409, saving model to nextword1.h5\n",
      "23/23 [==============================] - 4s 175ms/step - loss: 1.4641\n",
      "Epoch 22/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4619\n",
      "Epoch 00022: loss improved from 1.46409 to 1.46188, saving model to nextword1.h5\n",
      "23/23 [==============================] - 4s 162ms/step - loss: 1.4619\n",
      "Epoch 23/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4588\n",
      "Epoch 00023: loss improved from 1.46188 to 1.45877, saving model to nextword1.h5\n",
      "23/23 [==============================] - 4s 178ms/step - loss: 1.4588\n",
      "Epoch 24/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4574\n",
      "Epoch 00024: loss improved from 1.45877 to 1.45738, saving model to nextword1.h5\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 1.4574\n",
      "Epoch 25/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4577\n",
      "Epoch 00025: loss did not improve from 1.45738\n",
      "23/23 [==============================] - 4s 159ms/step - loss: 1.4577\n",
      "Epoch 26/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4567\n",
      "Epoch 00026: loss improved from 1.45738 to 1.45673, saving model to nextword1.h5\n",
      "23/23 [==============================] - 4s 176ms/step - loss: 1.4567\n",
      "Epoch 27/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4552\n",
      "Epoch 00027: loss improved from 1.45673 to 1.45516, saving model to nextword1.h5\n",
      "23/23 [==============================] - 4s 172ms/step - loss: 1.4552\n",
      "Epoch 28/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4525- ETA: \n",
      "Epoch 00028: loss improved from 1.45516 to 1.45249, saving model to nextword1.h5\n",
      "23/23 [==============================] - 4s 185ms/step - loss: 1.4525\n",
      "Epoch 29/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4502\n",
      "Epoch 00029: loss improved from 1.45249 to 1.45021, saving model to nextword1.h5\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 1.4502\n",
      "Epoch 30/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4535\n",
      "Epoch 00030: loss did not improve from 1.45021\n",
      "23/23 [==============================] - 4s 153ms/step - loss: 1.4535\n",
      "Epoch 31/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4512\n",
      "Epoch 00031: loss did not improve from 1.45021\n",
      "23/23 [==============================] - 4s 174ms/step - loss: 1.4512\n",
      "Epoch 32/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4492\n",
      "Epoch 00032: loss improved from 1.45021 to 1.44924, saving model to nextword1.h5\n",
      "23/23 [==============================] - 4s 178ms/step - loss: 1.4492\n",
      "Epoch 33/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4499\n",
      "Epoch 00033: loss did not improve from 1.44924\n",
      "23/23 [==============================] - 4s 169ms/step - loss: 1.4499\n",
      "Epoch 34/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4515\n",
      "Epoch 00034: loss did not improve from 1.44924\n",
      "23/23 [==============================] - 3s 145ms/step - loss: 1.4515\n",
      "Epoch 35/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4433\n",
      "Epoch 00035: loss improved from 1.44924 to 1.44332, saving model to nextword1.h5\n",
      "23/23 [==============================] - 4s 187ms/step - loss: 1.4433\n",
      "Epoch 36/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4439\n",
      "Epoch 00036: loss did not improve from 1.44332\n",
      "23/23 [==============================] - 4s 167ms/step - loss: 1.4439\n",
      "Epoch 37/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4467\n",
      "Epoch 00037: loss did not improve from 1.44332\n",
      "23/23 [==============================] - 4s 153ms/step - loss: 1.4467\n",
      "Epoch 38/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4458\n",
      "Epoch 00038: loss did not improve from 1.44332\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
      "23/23 [==============================] - 3s 151ms/step - loss: 1.4458\n",
      "Epoch 39/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4244\n",
      "Epoch 00039: loss improved from 1.44332 to 1.42441, saving model to nextword1.h5\n",
      "23/23 [==============================] - 4s 166ms/step - loss: 1.4244\n",
      "Epoch 40/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4201\n",
      "Epoch 00040: loss improved from 1.42441 to 1.42005, saving model to nextword1.h5\n",
      "23/23 [==============================] - 4s 192ms/step - loss: 1.4201\n",
      "Epoch 41/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4193\n",
      "Epoch 00041: loss improved from 1.42005 to 1.41927, saving model to nextword1.h5\n",
      "23/23 [==============================] - 4s 171ms/step - loss: 1.4193\n",
      "Epoch 42/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4192\n",
      "Epoch 00042: loss improved from 1.41927 to 1.41920, saving model to nextword1.h5\n",
      "23/23 [==============================] - 4s 174ms/step - loss: 1.4192\n",
      "Epoch 43/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4165\n",
      "Epoch 00043: loss improved from 1.41920 to 1.41653, saving model to nextword1.h5\n",
      "23/23 [==============================] - 4s 184ms/step - loss: 1.4165\n",
      "Epoch 44/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4166\n",
      "Epoch 00044: loss did not improve from 1.41653\n",
      "23/23 [==============================] - 4s 165ms/step - loss: 1.4166\n",
      "Epoch 45/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4151\n",
      "Epoch 00045: loss improved from 1.41653 to 1.41511, saving model to nextword1.h5\n",
      "23/23 [==============================] - 4s 174ms/step - loss: 1.4151\n",
      "Epoch 46/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4142\n",
      "Epoch 00046: loss improved from 1.41511 to 1.41425, saving model to nextword1.h5\n",
      "23/23 [==============================] - 4s 154ms/step - loss: 1.4142\n",
      "Epoch 47/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4139\n",
      "Epoch 00047: loss improved from 1.41425 to 1.41390, saving model to nextword1.h5\n",
      "23/23 [==============================] - 4s 180ms/step - loss: 1.4139\n",
      "Epoch 48/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4150\n",
      "Epoch 00048: loss did not improve from 1.41390\n",
      "23/23 [==============================] - 3s 147ms/step - loss: 1.4150\n",
      "Epoch 49/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4130\n",
      "Epoch 00049: loss improved from 1.41390 to 1.41303, saving model to nextword1.h5\n",
      "23/23 [==============================] - 4s 170ms/step - loss: 1.4130\n",
      "Epoch 50/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4135\n",
      "Epoch 00050: loss did not improve from 1.41303\n",
      "23/23 [==============================] - 3s 145ms/step - loss: 1.4135\n",
      "Epoch 51/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4132\n",
      "Epoch 00051: loss did not improve from 1.41303\n",
      "23/23 [==============================] - 4s 155ms/step - loss: 1.4132\n",
      "Epoch 52/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4129\n",
      "Epoch 00052: loss improved from 1.41303 to 1.41289, saving model to nextword1.h5\n",
      "23/23 [==============================] - 4s 189ms/step - loss: 1.4129\n",
      "Epoch 53/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4125\n",
      "Epoch 00053: loss improved from 1.41289 to 1.41247, saving model to nextword1.h5\n",
      "23/23 [==============================] - 4s 164ms/step - loss: 1.4125\n",
      "Epoch 54/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4127\n",
      "Epoch 00054: loss did not improve from 1.41247\n",
      "23/23 [==============================] - 4s 159ms/step - loss: 1.4127\n",
      "Epoch 55/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4112\n",
      "Epoch 00055: loss improved from 1.41247 to 1.41120, saving model to nextword1.h5\n",
      "23/23 [==============================] - 4s 178ms/step - loss: 1.4112\n",
      "Epoch 56/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4113\n",
      "Epoch 00056: loss did not improve from 1.41120\n",
      "23/23 [==============================] - 4s 169ms/step - loss: 1.4113\n",
      "Epoch 57/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4121\n",
      "Epoch 00057: loss did not improve from 1.41120\n",
      "23/23 [==============================] - 4s 163ms/step - loss: 1.4121\n",
      "Epoch 58/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4108\n",
      "Epoch 00058: loss improved from 1.41120 to 1.41080, saving model to nextword1.h5\n",
      "23/23 [==============================] - 4s 157ms/step - loss: 1.4108\n",
      "Epoch 59/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4093\n",
      "Epoch 00059: loss improved from 1.41080 to 1.40926, saving model to nextword1.h5\n",
      "23/23 [==============================] - 4s 178ms/step - loss: 1.4093\n",
      "Epoch 60/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4109\n",
      "Epoch 00060: loss did not improve from 1.40926\n",
      "23/23 [==============================] - 3s 149ms/step - loss: 1.4109\n",
      "Epoch 61/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4105\n",
      "Epoch 00061: loss did not improve from 1.40926\n",
      "23/23 [==============================] - 4s 156ms/step - loss: 1.4105\n",
      "Epoch 62/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4092\n",
      "Epoch 00062: loss improved from 1.40926 to 1.40919, saving model to nextword1.h5\n",
      "23/23 [==============================] - 4s 154ms/step - loss: 1.4092\n",
      "Epoch 63/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4099\n",
      "Epoch 00063: loss did not improve from 1.40919\n",
      "23/23 [==============================] - 4s 154ms/step - loss: 1.4099\n",
      "Epoch 64/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4080\n",
      "Epoch 00064: loss improved from 1.40919 to 1.40796, saving model to nextword1.h5\n",
      "23/23 [==============================] - 4s 174ms/step - loss: 1.4080\n",
      "Epoch 65/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4088\n",
      "Epoch 00065: loss did not improve from 1.40796\n",
      "23/23 [==============================] - 3s 146ms/step - loss: 1.4088\n",
      "Epoch 66/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4066\n",
      "Epoch 00066: loss improved from 1.40796 to 1.40665, saving model to nextword1.h5\n",
      "23/23 [==============================] - 4s 164ms/step - loss: 1.4066\n",
      "Epoch 67/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4091\n",
      "Epoch 00067: loss did not improve from 1.40665\n",
      "23/23 [==============================] - 3s 149ms/step - loss: 1.4091\n",
      "Epoch 68/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4069\n",
      "Epoch 00068: loss did not improve from 1.40665\n",
      "23/23 [==============================] - 4s 173ms/step - loss: 1.4069\n",
      "Epoch 69/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4067\n",
      "Epoch 00069: loss did not improve from 1.40665\n",
      "23/23 [==============================] - 3s 149ms/step - loss: 1.4067\n",
      "Epoch 70/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4078\n",
      "Epoch 00070: loss did not improve from 1.40665\n",
      "23/23 [==============================] - 4s 163ms/step - loss: 1.4078\n",
      "Epoch 71/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4058\n",
      "Epoch 00071: loss improved from 1.40665 to 1.40576, saving model to nextword1.h5\n",
      "23/23 [==============================] - 4s 173ms/step - loss: 1.4058\n",
      "Epoch 72/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4056\n",
      "Epoch 00072: loss improved from 1.40576 to 1.40556, saving model to nextword1.h5\n",
      "23/23 [==============================] - 4s 176ms/step - loss: 1.4056\n",
      "Epoch 73/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4061\n",
      "Epoch 00073: loss did not improve from 1.40556\n",
      "23/23 [==============================] - 4s 158ms/step - loss: 1.4061\n",
      "Epoch 74/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4052\n",
      "Epoch 00074: loss improved from 1.40556 to 1.40525, saving model to nextword1.h5\n",
      "23/23 [==============================] - 4s 157ms/step - loss: 1.4052\n",
      "Epoch 75/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4069\n",
      "Epoch 00075: loss did not improve from 1.40525\n",
      "23/23 [==============================] - 4s 154ms/step - loss: 1.4069\n",
      "Epoch 76/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4051\n",
      "Epoch 00076: loss improved from 1.40525 to 1.40508, saving model to nextword1.h5\n",
      "23/23 [==============================] - 4s 177ms/step - loss: 1.4051\n",
      "Epoch 77/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4080- ETA: 0s - loss: 1.409\n",
      "Epoch 00077: loss did not improve from 1.40508\n",
      "23/23 [==============================] - 4s 154ms/step - loss: 1.4080\n",
      "Epoch 78/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4041- ETA: 1s - l\n",
      "Epoch 00078: loss improved from 1.40508 to 1.40410, saving model to nextword1.h5\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 1.4041\n",
      "Epoch 79/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4034\n",
      "Epoch 00079: loss improved from 1.40410 to 1.40338, saving model to nextword1.h5\n",
      "23/23 [==============================] - 4s 160ms/step - loss: 1.4034\n",
      "Epoch 80/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4031\n",
      "Epoch 00080: loss improved from 1.40338 to 1.40306, saving model to nextword1.h5\n",
      "23/23 [==============================] - 4s 195ms/step - loss: 1.4031\n",
      "Epoch 81/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4037\n",
      "Epoch 00081: loss did not improve from 1.40306\n",
      "23/23 [==============================] - 4s 162ms/step - loss: 1.4037\n",
      "Epoch 82/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4057\n",
      "Epoch 00082: loss did not improve from 1.40306\n",
      "23/23 [==============================] - 3s 145ms/step - loss: 1.4057\n",
      "Epoch 83/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4038\n",
      "Epoch 00083: loss did not improve from 1.40306\n",
      "23/23 [==============================] - 3s 147ms/step - loss: 1.4038\n",
      "Epoch 84/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4045\n",
      "Epoch 00084: loss did not improve from 1.40306\n",
      "23/23 [==============================] - 4s 156ms/step - loss: 1.4045\n",
      "Epoch 85/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4045\n",
      "Epoch 00085: loss did not improve from 1.40306\n",
      "23/23 [==============================] - 4s 153ms/step - loss: 1.4045\n",
      "Epoch 86/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4030\n",
      "Epoch 00086: loss improved from 1.40306 to 1.40297, saving model to nextword1.h5\n",
      "23/23 [==============================] - 3s 150ms/step - loss: 1.4030\n",
      "Epoch 87/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4019\n",
      "Epoch 00087: loss improved from 1.40297 to 1.40190, saving model to nextword1.h5\n",
      "23/23 [==============================] - 4s 163ms/step - loss: 1.4019\n",
      "Epoch 88/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4019\n",
      "Epoch 00088: loss improved from 1.40190 to 1.40188, saving model to nextword1.h5\n",
      "23/23 [==============================] - 4s 157ms/step - loss: 1.4019\n",
      "Epoch 89/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4008\n",
      "Epoch 00089: loss improved from 1.40188 to 1.40075, saving model to nextword1.h5\n",
      "23/23 [==============================] - 4s 167ms/step - loss: 1.4008\n",
      "Epoch 90/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4023\n",
      "Epoch 00090: loss did not improve from 1.40075\n",
      "23/23 [==============================] - 3s 143ms/step - loss: 1.4023\n",
      "Epoch 91/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4009\n",
      "Epoch 00091: loss did not improve from 1.40075\n",
      "23/23 [==============================] - 4s 155ms/step - loss: 1.4009\n",
      "Epoch 92/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4007\n",
      "Epoch 00092: loss improved from 1.40075 to 1.40067, saving model to nextword1.h5\n",
      "23/23 [==============================] - 4s 159ms/step - loss: 1.4007\n",
      "Epoch 93/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4018\n",
      "Epoch 00093: loss did not improve from 1.40067\n",
      "23/23 [==============================] - 4s 154ms/step - loss: 1.4018\n",
      "Epoch 94/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.3991\n",
      "Epoch 00094: loss improved from 1.40067 to 1.39911, saving model to nextword1.h5\n",
      "23/23 [==============================] - 4s 167ms/step - loss: 1.3991\n",
      "Epoch 95/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4001\n",
      "Epoch 00095: loss did not improve from 1.39911\n",
      "23/23 [==============================] - 3s 141ms/step - loss: 1.4001\n",
      "Epoch 96/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4017\n",
      "Epoch 00096: loss did not improve from 1.39911\n",
      "23/23 [==============================] - 3s 151ms/step - loss: 1.4017\n",
      "Epoch 97/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.3989\n",
      "Epoch 00097: loss improved from 1.39911 to 1.39892, saving model to nextword1.h5\n",
      "23/23 [==============================] - 4s 164ms/step - loss: 1.3989\n",
      "Epoch 98/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.3986\n",
      "Epoch 00098: loss improved from 1.39892 to 1.39862, saving model to nextword1.h5\n",
      "23/23 [==============================] - 4s 162ms/step - loss: 1.3986\n",
      "Epoch 99/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.3996\n",
      "Epoch 00099: loss did not improve from 1.39862\n",
      "23/23 [==============================] - 3s 148ms/step - loss: 1.3996\n",
      "Epoch 100/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.3994\n",
      "Epoch 00100: loss did not improve from 1.39862\n",
      "23/23 [==============================] - 3s 142ms/step - loss: 1.3994\n",
      "Epoch 101/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4013\n",
      "Epoch 00101: loss did not improve from 1.39862\n",
      "23/23 [==============================] - 4s 157ms/step - loss: 1.4013\n",
      "Epoch 102/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4019\n",
      "Epoch 00102: loss did not improve from 1.39862\n",
      "23/23 [==============================] - 3s 147ms/step - loss: 1.4019\n",
      "Epoch 103/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4004\n",
      "Epoch 00103: loss did not improve from 1.39862\n",
      "23/23 [==============================] - 4s 154ms/step - loss: 1.4004\n",
      "Epoch 104/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.3987\n",
      "Epoch 00104: loss did not improve from 1.39862\n",
      "23/23 [==============================] - 3s 141ms/step - loss: 1.3987\n",
      "Epoch 105/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.3996\n",
      "Epoch 00105: loss did not improve from 1.39862\n",
      "23/23 [==============================] - 3s 151ms/step - loss: 1.3996\n",
      "Epoch 106/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.3986\n",
      "Epoch 00106: loss improved from 1.39862 to 1.39861, saving model to nextword1.h5\n",
      "23/23 [==============================] - 4s 163ms/step - loss: 1.3986\n",
      "Epoch 107/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.3996\n",
      "Epoch 00107: loss did not improve from 1.39861\n",
      "23/23 [==============================] - 4s 154ms/step - loss: 1.3996\n",
      "Epoch 108/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.3997\n",
      "Epoch 00108: loss did not improve from 1.39861\n",
      "23/23 [==============================] - 4s 165ms/step - loss: 1.3997\n",
      "Epoch 109/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.3998\n",
      "Epoch 00109: loss did not improve from 1.39861\n",
      "23/23 [==============================] - 4s 160ms/step - loss: 1.3998\n",
      "Epoch 110/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.3998\n",
      "Epoch 00110: loss did not improve from 1.39861\n",
      "23/23 [==============================] - 4s 170ms/step - loss: 1.3998\n",
      "Epoch 111/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.3999\n",
      "Epoch 00111: loss did not improve from 1.39861\n",
      "23/23 [==============================] - 3s 150ms/step - loss: 1.3999\n",
      "Epoch 112/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.3992\n",
      "Epoch 00112: loss did not improve from 1.39861\n",
      "23/23 [==============================] - 3s 150ms/step - loss: 1.3992\n",
      "Epoch 113/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.3982\n",
      "Epoch 00113: loss improved from 1.39861 to 1.39816, saving model to nextword1.h5\n",
      "23/23 [==============================] - 4s 158ms/step - loss: 1.3982\n",
      "Epoch 114/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.3970\n",
      "Epoch 00114: loss improved from 1.39816 to 1.39696, saving model to nextword1.h5\n",
      "23/23 [==============================] - 4s 186ms/step - loss: 1.3970\n",
      "Epoch 115/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.3962\n",
      "Epoch 00115: loss improved from 1.39696 to 1.39618, saving model to nextword1.h5\n",
      "23/23 [==============================] - 4s 163ms/step - loss: 1.3962\n",
      "Epoch 116/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.3969\n",
      "Epoch 00116: loss did not improve from 1.39618\n",
      "23/23 [==============================] - 4s 155ms/step - loss: 1.3969\n",
      "Epoch 117/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.3968\n",
      "Epoch 00117: loss did not improve from 1.39618\n",
      "23/23 [==============================] - 4s 156ms/step - loss: 1.3968\n",
      "Epoch 118/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.3966\n",
      "Epoch 00118: loss did not improve from 1.39618\n",
      "23/23 [==============================] - 4s 164ms/step - loss: 1.3966\n",
      "Epoch 119/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4007\n",
      "Epoch 00119: loss did not improve from 1.39618\n",
      "23/23 [==============================] - 4s 161ms/step - loss: 1.4007\n",
      "Epoch 120/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.3966\n",
      "Epoch 00120: loss did not improve from 1.39618\n",
      "23/23 [==============================] - 3s 151ms/step - loss: 1.3966\n",
      "Epoch 121/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.3965\n",
      "Epoch 00121: loss did not improve from 1.39618\n",
      "23/23 [==============================] - 3s 150ms/step - loss: 1.3965\n",
      "Epoch 122/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.3957\n",
      "Epoch 00122: loss improved from 1.39618 to 1.39574, saving model to nextword1.h5\n",
      "23/23 [==============================] - 4s 184ms/step - loss: 1.3957\n",
      "Epoch 123/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.3953\n",
      "Epoch 00123: loss improved from 1.39574 to 1.39526, saving model to nextword1.h5\n",
      "23/23 [==============================] - 4s 169ms/step - loss: 1.3953\n",
      "Epoch 124/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.3954\n",
      "Epoch 00124: loss did not improve from 1.39526\n",
      "23/23 [==============================] - 3s 150ms/step - loss: 1.3954\n",
      "Epoch 125/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.3966\n",
      "Epoch 00125: loss did not improve from 1.39526\n",
      "23/23 [==============================] - 3s 147ms/step - loss: 1.3966\n",
      "Epoch 126/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.3961\n",
      "Epoch 00126: loss did not improve from 1.39526\n",
      "23/23 [==============================] - 4s 158ms/step - loss: 1.3961\n",
      "Epoch 127/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.3961\n",
      "Epoch 00127: loss did not improve from 1.39526\n",
      "23/23 [==============================] - 4s 156ms/step - loss: 1.3961\n",
      "Epoch 128/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.3969\n",
      "Epoch 00128: loss did not improve from 1.39526\n",
      "23/23 [==============================] - 4s 168ms/step - loss: 1.3969\n",
      "Epoch 129/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.3955\n",
      "Epoch 00129: loss did not improve from 1.39526\n",
      "23/23 [==============================] - 4s 158ms/step - loss: 1.3955\n",
      "Epoch 130/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.3956\n",
      "Epoch 00130: loss did not improve from 1.39526\n",
      "23/23 [==============================] - 4s 163ms/step - loss: 1.3956\n",
      "Epoch 131/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.3970\n",
      "Epoch 00131: loss did not improve from 1.39526\n",
      "23/23 [==============================] - 4s 168ms/step - loss: 1.3970\n",
      "Epoch 132/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.3957\n",
      "Epoch 00132: loss did not improve from 1.39526\n",
      "23/23 [==============================] - 4s 153ms/step - loss: 1.3957\n",
      "Epoch 133/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.3948\n",
      "Epoch 00133: loss improved from 1.39526 to 1.39479, saving model to nextword1.h5\n",
      "23/23 [==============================] - 4s 170ms/step - loss: 1.3948\n",
      "Epoch 134/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.3933\n",
      "Epoch 00134: loss improved from 1.39479 to 1.39333, saving model to nextword1.h5\n",
      "23/23 [==============================] - 4s 181ms/step - loss: 1.3933\n",
      "Epoch 135/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.3942\n",
      "Epoch 00135: loss did not improve from 1.39333\n",
      "23/23 [==============================] - 4s 169ms/step - loss: 1.3942\n",
      "Epoch 136/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.3947\n",
      "Epoch 00136: loss did not improve from 1.39333\n",
      "23/23 [==============================] - 4s 168ms/step - loss: 1.3947\n",
      "Epoch 137/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.3951\n",
      "Epoch 00137: loss did not improve from 1.39333\n",
      "23/23 [==============================] - 3s 151ms/step - loss: 1.3951\n",
      "Epoch 138/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.3937\n",
      "Epoch 00138: loss did not improve from 1.39333\n",
      "23/23 [==============================] - 4s 164ms/step - loss: 1.3937\n",
      "Epoch 139/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.3942\n",
      "Epoch 00139: loss did not improve from 1.39333\n",
      "23/23 [==============================] - 4s 171ms/step - loss: 1.3942\n",
      "Epoch 140/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.3942\n",
      "Epoch 00140: loss did not improve from 1.39333\n",
      "23/23 [==============================] - 4s 153ms/step - loss: 1.3942\n",
      "Epoch 141/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.3929\n",
      "Epoch 00141: loss improved from 1.39333 to 1.39294, saving model to nextword1.h5\n",
      "23/23 [==============================] - 4s 170ms/step - loss: 1.3929\n",
      "Epoch 142/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.3949\n",
      "Epoch 00142: loss did not improve from 1.39294\n",
      "23/23 [==============================] - 4s 161ms/step - loss: 1.3949\n",
      "Epoch 143/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.3935\n",
      "Epoch 00143: loss did not improve from 1.39294\n",
      "23/23 [==============================] - 4s 182ms/step - loss: 1.3935\n",
      "Epoch 144/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.3925\n",
      "Epoch 00144: loss improved from 1.39294 to 1.39253, saving model to nextword1.h5\n",
      "23/23 [==============================] - 4s 176ms/step - loss: 1.3925\n",
      "Epoch 145/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.3948\n",
      "Epoch 00145: loss did not improve from 1.39253\n",
      "23/23 [==============================] - 4s 156ms/step - loss: 1.3948\n",
      "Epoch 146/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.3931\n",
      "Epoch 00146: loss did not improve from 1.39253\n",
      "23/23 [==============================] - 4s 168ms/step - loss: 1.3931\n",
      "Epoch 147/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.3958\n",
      "Epoch 00147: loss did not improve from 1.39253\n",
      "23/23 [==============================] - 4s 166ms/step - loss: 1.3958\n",
      "Epoch 148/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.3922\n",
      "Epoch 00148: loss improved from 1.39253 to 1.39218, saving model to nextword1.h5\n",
      "23/23 [==============================] - 4s 178ms/step - loss: 1.3922\n",
      "Epoch 149/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.3941\n",
      "Epoch 00149: loss did not improve from 1.39218\n",
      "23/23 [==============================] - 4s 161ms/step - loss: 1.3941\n",
      "Epoch 150/150\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.3936\n",
      "Epoch 00150: loss did not improve from 1.39218\n",
      "23/23 [==============================] - 3s 152ms/step - loss: 1.3936\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2243b6a6c88>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x, y, epochs=150, batch_size=64,callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorBoard plus Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-07T10:58:08.948467Z",
     "start_time": "2020-11-07T10:57:08.698442Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Timed out waiting for TensorBoard to start. It may still be running as pid 12120."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#%load_ext tensorboard.notebook\n",
    "\n",
    "%reload_ext tensorboard\n",
    "#  %reload_ext tensorboard\n",
    "#--debugger_port <port_number>\n",
    "\n",
    "%tensorboard --logdir=./logsnextword #logs/fit\n",
    "\n",
    "#ERROR: Timed out waiting for TensorBoard to start. It may still be running as pid 12120."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-07T10:59:06.799253Z",
     "start_time": "2020-11-07T10:59:01.273045Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Failed to launch TensorBoard (exited with 1).\n",
       "Contents of stderr:\n",
       "2020-11-07 16:29:03.557816: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\n",
       "2020-11-07 16:29:03.560310: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
       "\n",
       "***** TensorBoard Uploader *****\n",
       "\n",
       "This will upload your TensorBoard logs to https://tensorboard.dev/ from\n",
       "the following directory:\n",
       "\n",
       "./logsnextword\n",
       "\n",
       "This TensorBoard will be visible to everyone. Do not upload sensitive\n",
       "data.\n",
       "\n",
       "Your use of this service is subject to Google's Terms of Service\n",
       "<https://policies.google.com/terms> and Privacy Policy\n",
       "<https://policies.google.com/privacy>, and TensorBoard.dev's Terms of Service\n",
       "<https://tensorboard.dev/policy/terms/>.\n",
       "\n",
       "This notice will not be shown again while you are logged into the uploader.\n",
       "To log out, run `tensorboard dev auth revoke`.\n",
       "\n",
       "Traceback (most recent call last):\n",
       "  File \"c:\\users\\admin\\anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n",
       "    \"__main__\", mod_spec)\n",
       "  File \"c:\\users\\admin\\anaconda3\\lib\\runpy.py\", line 85, in _run_code\n",
       "    exec(code, run_globals)\n",
       "  File \"C:\\Users\\Admin\\anaconda3\\Scripts\\tensorboard.exe\\__main__.py\", line 7, in <module>\n",
       "  File \"c:\\users\\admin\\anaconda3\\lib\\site-packages\\tensorboard\\main.py\", line 75, in run_main\n",
       "    app.run(tensorboard.main, flags_parser=tensorboard.configure)\n",
       "  File \"c:\\users\\admin\\anaconda3\\lib\\site-packages\\absl\\app.py\", line 299, in run\n",
       "    _run_main(main, args)\n",
       "  File \"c:\\users\\admin\\anaconda3\\lib\\site-packages\\absl\\app.py\", line 250, in _run_main\n",
       "    sys.exit(main(argv))\n",
       "  File \"c:\\users\\admin\\anaconda3\\lib\\site-packages\\tensorboard\\program.py\", line 290, in main\n",
       "    return runner(self.flags) or 0\n",
       "  File \"c:\\users\\admin\\anaconda3\\lib\\site-packages\\tensorboard\\uploader\\uploader_subcommand.py\", line 633, in run\n",
       "    return _run(flags)\n",
       "  File \"c:\\users\\admin\\anaconda3\\lib\\site-packages\\tensorboard\\uploader\\uploader_subcommand.py\", line 97, in _run\n",
       "    _prompt_for_user_ack(intent)\n",
       "  File \"c:\\users\\admin\\anaconda3\\lib\\site-packages\\tensorboard\\uploader\\uploader_subcommand.py\", line 72, in _prompt_for_user_ack\n",
       "    response = six.moves.input(\"Continue? (yes/NO) \")\n",
       "EOFError: EOF when reading a line\n",
       "Contents of stdout:\n",
       "Continue? (yes/NO)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard dev upload --logdir ./logsnextword\n",
    "#\\\n",
    " #   'logs/fit'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-07T10:49:22.536437Z",
     "start_time": "2020-11-07T10:49:20.952430Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-44-70c25d3d078c>:1: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 27,  33,   3, ..., 111, 550, 551], dtype=int64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model.predict_classes(x)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-07T10:50:34.604425Z",
     "start_time": "2020-11-07T10:50:33.008695Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 1s 18ms/step - loss: 69.1827\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "69.18267822265625"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(prediction,y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To predict the next word-  we have to decide by taking  the input as one word to predict one word i.e output\n",
    "## or 2 words to predict one and so onnn.. so for that we need n-grams - unigram bigram and trigram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## model takes index and gives index your responsibility to know the word at that index\n",
    "\n",
    "## tokenization act as a dictionary which helps in prediction purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-06T12:58:23.339944Z",
     "start_time": "2020-11-06T12:58:23.334949Z"
    }
   },
   "outputs": [],
   "source": [
    "## setting path for plotting model\n",
    "#import os\n",
    "\n",
    "#os.environ[\"Path\"]= \"C:\\\\Program Files\\\\Graphviz\\\\bin\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-06T12:58:42.550987Z",
     "start_time": "2020-11-06T12:58:42.545001Z"
    }
   },
   "outputs": [],
   "source": [
    "#import os\n",
    "#os.getenv(\"Path\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## n grams - unigram - for every word what is the next word\n",
    "##bigram - using two words what is the next word\n",
    "\n",
    "\n",
    "\n",
    "#[17,53,293,2,.....]\n",
    "#[  [17 53]   [53 293]  [293 2]]\n",
    "\n",
    "## in trigram 17,53,293 become one record \n",
    "##feature space is same but input space shrinks so size of training data decreases"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
